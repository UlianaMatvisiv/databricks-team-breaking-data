{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4b93be-a69f-4e12-95b1-8291c1a411b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def collect_quality_metrics(schema_name=\"silver\"):\n",
    "    tables = spark.sql(f\"SHOW TABLES IN first_phase.{schema_name}\").collect()\n",
    "    metrics = []\n",
    "\n",
    "    for table_row in tables:\n",
    "        table_name = table_row.tableName\n",
    "        full_table = f\"first_phase.{schema_name}.{table_name}\"\n",
    "        df = spark.table(full_table)\n",
    "\n",
    "        # Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ð° ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ñ€ÑÐ´ÐºÑ–Ð²\n",
    "        total_count = df.count()\n",
    "\n",
    "        # ÐžÑ‚Ñ€Ð¸Ð¼ÑƒÑ”Ð¼Ð¾ Ð¿Ð¾Ð¿ÐµÑ€ÐµÐ´Ð½Ñ” Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ row_count, ÑÐºÑ‰Ð¾ Ñ–ÑÐ½ÑƒÑ”\n",
    "        prev = spark.sql(f\"\"\"\n",
    "            SELECT metric_value \n",
    "            FROM first_phase.monitoring.data_quality_metrics\n",
    "            WHERE table_name = '{table_name}' \n",
    "              AND metric_name = 'total_rows'\n",
    "            ORDER BY check_timestamp DESC\n",
    "            LIMIT 1\n",
    "        \"\"\").collect()\n",
    "\n",
    "        prev_count = prev[0][\"metric_value\"] if prev else None\n",
    "        diff_pct = abs(total_count - prev_count) / prev_count * 100 if prev_count and prev_count > 0 else 0\n",
    "        status = \"WARNING\" if diff_pct > 20 else \"OK\"\n",
    "\n",
    "        metrics.append({\n",
    "            'table_name': table_name,\n",
    "            'metric_name': 'total_rows',\n",
    "            'metric_value': float(total_count),\n",
    "            'check_timestamp': datetime.now(),\n",
    "            'status': status,\n",
    "            'details': f'Row count changed by {diff_pct:.2f}% from last check' if prev_count else 'Initial check'\n",
    "        })\n",
    "\n",
    "        # NULL Ñƒ ÐºÐ»ÑŽÑ‡Ð¾Ð²Ð¸Ñ… Ð¿Ð¾Ð»ÑÑ…\n",
    "        for column in df.columns:\n",
    "            if 'key' in column.lower() or column.endswith('_id'):\n",
    "                null_count = df.filter(F.col(column).isNull()).count()\n",
    "                null_pct = (null_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "                metrics.append({\n",
    "                    'table_name': table_name,\n",
    "                    'metric_name': f'null_pct_{column}',\n",
    "                    'metric_value': float(null_pct),\n",
    "                    'check_timestamp': datetime.now(),\n",
    "                    'status': 'ERROR' if null_count > 0 else 'OK',\n",
    "                    'details': f'{null_count} null values found in {column}'\n",
    "                })\n",
    "\n",
    "        #  ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð´ÑƒÐ±Ð»Ñ–ÐºÐ¾Ð²Ð°Ð½Ð¸Ñ… PK\n",
    "\n",
    "        pk_cols = [c for c in df.columns if c.lower().endswith('key') or c.endswith('_id')]\n",
    "        if len(pk_cols) == 1:  # ÑÐºÑ‰Ð¾ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ– Ð¾Ð´Ð¸Ð½ PK\n",
    "            pk_col = pk_cols[0]\n",
    "            dup_count = df.groupBy(pk_col).count().filter(\"count > 1\").count()\n",
    "\n",
    "            metrics.append({\n",
    "                'table_name': table_name,\n",
    "                'metric_name': f'duplicated_{pk_col}',\n",
    "                'metric_value': float(dup_count),\n",
    "                'check_timestamp': datetime.now(),\n",
    "                'status': 'ERROR' if dup_count > 0 else 'OK',\n",
    "                'details': f'{dup_count} duplicated primary keys in {pk_col}'\n",
    "            })\n",
    "\n",
    "    # Ð—Ð°Ð¿Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ñƒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑŽ\n",
    "    metrics_df = spark.createDataFrame(metrics)\n",
    "    metrics_df.write.mode(\"append\").saveAsTable(\"first_phase.monitoring.data_quality_metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f611b6-84a4-4dd4-877c-7769a4fac114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def capture_schema_snapshot(schema_name=\"silver\"):\n",
    "    \"\"\"\n",
    "    Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ snapshot ÑÑ…ÐµÐ¼Ð¸ Ð´Ð»Ñ Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð½Ñ Ð·Ð¼Ñ–Ð½\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“¸ Capturing schema snapshot for {schema_name}...\")\n",
    "    \n",
    "    tables = spark.sql(f\"SHOW TABLES IN {CATALOG}.{schema_name}\").collect()\n",
    "    snapshot_count = 0\n",
    "\n",
    "    for table_row in tables:\n",
    "        table_name = table_row.tableName\n",
    "        columns = spark.table(f\"{CATALOG}.{schema_name}.{table_name}\").dtypes\n",
    "\n",
    "        for col_name, col_type in columns:\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO {CATALOG}.monitoring.schema_snapshots \n",
    "                VALUES ('{table_name}', '{col_name}', '{col_type}', current_timestamp())\n",
    "            \"\"\")\n",
    "            snapshot_count += 1\n",
    "    \n",
    "    print(f\"   âœ… Captured {snapshot_count} column definitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f456edcf-d430-4d05-aafc-67fd7c2dd153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_referential_integrity():\n",
    "    checks = [\n",
    "        (\"orders\", \"o_custkey\", \"customer\", \"c_custkey\"),\n",
    "        (\"lineitem\", \"l_orderkey\", \"orders\", \"o_orderkey\"),\n",
    "        (\"partsupp\", \"ps_partkey\", \"part\", \"p_partkey\"),\n",
    "        (\"supplier\", \"s_nationkey\", \"nation\", \"n_nationkey\"),\n",
    "    ]\n",
    "\n",
    "    for child_tbl, fk, parent_tbl, pk in checks:\n",
    "        orphaned = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) AS cnt\n",
    "            FROM first_phase.silver.{child_tbl} c\n",
    "            LEFT JOIN first_phase.silver.{parent_tbl} p\n",
    "              ON c.{fk} = p.{pk}\n",
    "            WHERE p.{pk} IS NULL\n",
    "        \"\"\").collect()[0]['cnt']\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO first_phase.monitoring.data_quality_metrics\n",
    "            VALUES (\n",
    "                '{child_tbl}',\n",
    "                'orphaned_fk_{fk}',\n",
    "                {float(orphaned)},\n",
    "                current_timestamp(),\n",
    "                '{'ERROR' if orphaned > 0 else 'OK'}',\n",
    "                'Found {orphaned} orphaned foreign keys from {child_tbl}.{fk} â†’ {parent_tbl}.{pk}'\n",
    "            )\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ffa8b9-93b5-4f54-aabf-50a321bba843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_load_info(schema, table, rows, duration, status):\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.monitoring.load_history \n",
    "        VALUES ('{schema}', '{table}', current_timestamp(), {rows}, {duration}, '{status}')\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e33f0c-33f1-4ba8-8aa1-8b97ac5606ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_monitoring():\n",
    "    print(\"Starting data quality monitoring...\")\n",
    "    collect_quality_metrics(\"bronze\")\n",
    "    collect_quality_metrics(\"silver\")\n",
    "    check_referential_integrity()\n",
    "    capture_schema_snapshot(\"silver\")\n",
    "    print(\"Monitoring completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03f7850-64c5-4dce-ab82-b9b32377fb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c790fa-c318-424a-a5dd-59da22900af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM first_phase.monitoring.data_quality_metrics ORDER BY check_timestamp DESC\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8454433928220775,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "monitoring_job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
