{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8064222d-278e-49cb-82ef-1c0708ac2d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring raw data from AirBnB listings from Paris and Venice\n",
    "## Team: Breaking data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d371822-f73f-4f41-aa83-8ae06a04270d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this notebook, we are working with the raw data from Venice and Paris AirBnB listings, exploring what we have, and making a sugestion on what can be cleaned and transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a023e4b-55aa-4c58-a9cc-8344585c0469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Overview on proposed actions \n",
    "\n",
    "**Data cleaning**: not all columns will bring a value for further analysis so we suggest dropping columns that contain urls to images, placements etc, as well as scraping info (id, time, source)\n",
    "\n",
    "**Transorming bathrooms column**: the column is empty for Paris and partially filled for Venice but in both cases it is possible to extract data from the batrooms_text column (suggestions are written in the notebook)\n",
    "\n",
    "**Hidden missing values**: in this notebook we already transformed N/A value into null, but there are also hidden values in amenities and host_verification columns that need to be taken care of (code that identifies such values available in the notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d62b17a-295c-40f7-958a-e7ddaae5f86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07df7087-3bd6-4ada-b9ec-abcd8d50d506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8981c47d-ea9e-4b3e-ae5a-b41988050c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/airbnb/airbnb_city_raw/city_data_volume/listings_paris.csv\"\n",
    "\n",
    "df_paris = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    .csv(path)\n",
    ")\n",
    "display(df_paris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a627296-a8c7-4ee7-aecc-a801d91c085e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/airbnb/airbnb_city_raw/city_data_volume/listings_venice.csv\"\n",
    "\n",
    "df_venice = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    .csv(path)\n",
    ")\n",
    "display(df_venice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5d66fd-4423-45f6-94f6-35c791e489d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, in the dataset we have information about the host (name, description, contact info, since when works etc), info on neighbourood of the host and the listing, information about the listing (bathrooms, rbedrooms, beds, price, amnenities, ratings, etc) and scraping information \n",
    "\n",
    "There are also columns that contain links to websites and images, which hold no significant value for the furhter research, which is why we suggest to drop it, as well as information about scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6537492-edcb-4e34-9556-6c80e32447a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8114f07-999c-44f3-a5ea-3303bd9c0077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listings_paris = df_paris.count()\n",
    "listings_venice = df_venice.count()\n",
    "\n",
    "names = [\"Paris\", \"Venice\"]\n",
    "values = [listings_paris, listings_venice]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.barh(names, values, color=\"#3D65A5\")\n",
    "\n",
    "ax.set_title(\"Number of AirBnB listings in Paris and Venice\")\n",
    "ax.set_ylabel(\"Number of Rows\")\n",
    "ax.set_xlim(xmax= max(values) * 1.15)\n",
    "\n",
    "# Add value lables\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + max(values) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{int(width):,}\",\n",
    "        ha=\"left\", va=\"center\", fontsize=10,\n",
    "    )\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2437f9-6c51-4b97-91aa-3b059244d62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So, as we can see, Paris has many more accommodations than Venice, which can be explained by the fact that Paris is a much bigger city and has simply more territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6749f9da-ddfe-4bf4-9623-44877dcdcf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "avg_paris = df_paris.select(F.avg(\"review_scores_value\")).collect()[0][0]\n",
    "avg_venice = df_venice.select(F.avg(\"review_scores_value\")).collect()[0][0]\n",
    "\n",
    "print(f\"Paris avg rating:  {avg_paris:.2f}\")\n",
    "print(f\"Venice avg rating: {avg_venice:.2f}\")\n",
    "\n",
    "names = [\"Paris\", \"Venice\"]\n",
    "values = [avg_paris, avg_venice]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.barh(names, values, color=\"#3D65A5\")\n",
    "\n",
    "ax.set_title(\"Average rate of AirBnB listings in Paris and Venice\")\n",
    "ax.set_ylabel(\"Average rating\")\n",
    "ax.set_ylabel(\"Number of Rows\")\n",
    "ax.set_xlim(xmax= max(values) * 1.1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(\n",
    "        width + max(values) * 0.01,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{width:.2f}\",\n",
    "        ha=\"left\", va=\"center\", fontsize=10,\n",
    "    )\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef2c59e2-a4f1-4d4c-aca8-41c604010723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In case of raitings, both cities have high review rates, but Paris is a bit lower in rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efb13d5-522b-4c12-9508-16e1a99b9f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_paris.withColumn(\"host_since\", F.to_date(\"host_since\"))\n",
    "\n",
    "df_clean = df_clean.withColumn(\"host_start_year\", F.year(\"host_since\"))\n",
    "\n",
    "hosts_by_year = (\n",
    "    df_clean\n",
    "    .filter(F.col(\"host_start_year\").isNotNull())\n",
    "    .groupBy(\"host_start_year\")\n",
    "    .agg(F.countDistinct(\"host_id\").alias(\"unique_hosts\"))\n",
    "    .orderBy(\"host_start_year\")\n",
    ")\n",
    "\n",
    "pdf = hosts_by_year.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(pdf[\"host_start_year\"].astype(str), pdf[\"unique_hosts\"], color=\"#3D65A5\")\n",
    "\n",
    "# Labels and title\n",
    "plt.title(\"Number of new host per year in Paris\", fontsize=14)\n",
    "plt.xlabel(\"Year host started\", fontsize=12)\n",
    "plt.ylabel(\"Unique hosts\", fontsize=12)\n",
    "plt.ylim(ymax=max(pdf[\"unique_hosts\"]) * 1.15)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height,                          \n",
    "        f\"{int(height)}\",              \n",
    "        ha=\"center\", va=\"bottom\", fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9281a025-68c1-4486-9d03-ad606eb0950c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_venice.withColumn(\"host_since\", F.to_date(\"host_since\"))\n",
    "\n",
    "df_clean = df_clean.withColumn(\"host_start_year\", F.year(\"host_since\"))\n",
    "\n",
    "hosts_by_year = (\n",
    "    df_clean\n",
    "    .filter(F.col(\"host_start_year\").isNotNull())\n",
    "    .groupBy(\"host_start_year\")\n",
    "    .agg(F.countDistinct(\"host_id\").alias(\"unique_hosts\"))\n",
    "    .orderBy(\"host_start_year\")\n",
    ")\n",
    "\n",
    "pdf = hosts_by_year.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(pdf[\"host_start_year\"].astype(str), pdf[\"unique_hosts\"], color=\"#3D65A5\")\n",
    "\n",
    "plt.title(\"Number of new host per year in Venice\", fontsize=14)\n",
    "plt.xlabel(\"Year host started\", fontsize=12)\n",
    "plt.ylabel(\"Unique hosts\", fontsize=12)\n",
    "plt.ylim(ymax=max(pdf[\"unique_hosts\"]) * 1.15)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height,                          \n",
    "        f\"{int(height)}\",              \n",
    "        ha=\"center\", va=\"bottom\", fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb3e74f-8adc-4281-9ea6-fabb57c56f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, in both cases the peak of new hosts was in 2015, probably because the platform became more popular, and the least amount of users was in 2020, which coincides with Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a77ed1a-3ad2-4654-8c36-090e57570d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_paris_clean  = df_paris.filter(F.col(\"host_id\").isNotNull())\n",
    "df_venice_clean = df_venice.filter(F.col(\"host_id\").isNotNull())\n",
    "\n",
    "# unique hosts\n",
    "unique_hosts_paris  = df_paris_clean.select(\"host_id\").distinct().count()\n",
    "unique_hosts_venice = df_venice_clean.select(\"host_id\").distinct().count()\n",
    "\n",
    "# listings per host (count of listings per host_id)\n",
    "paris_host_listing_counts = (\n",
    "    df_paris_clean.groupBy(\"host_id\")\n",
    "    .agg(F.count(\"*\").alias(\"listings_for_host\"))\n",
    ")\n",
    "avg_listings_per_host_paris = paris_host_listing_counts.select(\n",
    "    F.avg(\"listings_for_host\")\n",
    ").collect()[0][0]\n",
    "\n",
    "venice_host_listing_counts = (\n",
    "    df_venice_clean.groupBy(\"host_id\")\n",
    "    .agg(F.count(\"*\").alias(\"listings_for_host\"))\n",
    ")\n",
    "avg_listings_per_host_venice = venice_host_listing_counts.select(\n",
    "    F.avg(\"listings_for_host\")\n",
    ").collect()[0][0]\n",
    "\n",
    "cities = [\"Paris\", \"Venice\"]\n",
    "\n",
    "unique_vals = [unique_hosts_paris, unique_hosts_venice]\n",
    "\n",
    "# Unique hosts plot\n",
    "plt.figure(figsize=(6,4))\n",
    "bars1 = plt.bar(cities, unique_vals, color=\"#3D65A5\")\n",
    "plt.title(\"Number of unique hosts\")\n",
    "plt.ylabel(\"Unique hosts\")\n",
    "plt.ylim(ymax=max(unique_vals) * 1.15)\n",
    "\n",
    "# add value labels\n",
    "for bar in bars1:\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        h,\n",
    "        f\"{int(h)}\",\n",
    "        ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Avg listings per host plot\n",
    "avg_vals = [avg_listings_per_host_paris, avg_listings_per_host_venice]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "bars2 = plt.bar(cities, avg_vals, color=\"#3D65A5\")\n",
    "plt.title(\"Average listings per host\")\n",
    "plt.ylabel(\"Avg listings per host\")\n",
    "plt.ylim(ymax=max(avg_vals) * 1.15)\n",
    "\n",
    "# value lables\n",
    "for bar in bars2:\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        h,\n",
    "        f\"{h:.2f}\",\n",
    "        ha=\"center\", va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86640826-5d9b-4663-84ed-fa124a6e63ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, on average in Paris it is common to have only 1 listing per person, while in venice the average is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3764fbbe-d6e7-489a-9692-9de61d385da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Missing value analysis\n",
    "\n",
    "Lets look at a quality of our raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384745e8-4b91-400a-9dc6-2d2a2750e5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At first, lets replace N/A with null, as some columns have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e850f27-f24d-4297-a9d5-3a3f7d95cb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loop through all columns and replace \"N/A\" with null\n",
    "df_paris = df_paris.select([\n",
    "    F.when(F.col(c) == \"N/A\", None).otherwise(F.col(c)).alias(c)\n",
    "    for c in df_paris.columns\n",
    "])\n",
    "\n",
    "df_venice = df_venice.select([\n",
    "    F.when(F.col(c) == \"N/A\", None).otherwise(F.col(c)).alias(c)\n",
    "    for c in df_venice.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464a1c5a-00ae-4448-af64-5b3806ba8f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b834ac-4c79-4d84-b302-9430219b65c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_missing_expr(col_name, dtype, include_empty_str):\n",
    "    c = F.col(col_name)\n",
    "    base_null = c.isNull()\n",
    "    if isinstance(dtype, (T.FloatType, T.DoubleType)):\n",
    "        cond = base_null | F.isnan(c)\n",
    "    elif isinstance(dtype, T.StringType):\n",
    "        cond = base_null | (F.length(F.trim(c)) == 0 if include_empty_str else F.lit(False))\n",
    "    else:\n",
    "        cond = base_null\n",
    "    return F.when(cond, 1).otherwise(0)\n",
    "\n",
    "def missing_summary(df, columns=None, include_empty_str=True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns were given\")\n",
    "\n",
    "    total_rows = df.count()\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "\n",
    "    exprs = [F.sum(is_missing_expr(c, schema_map[c], include_empty_str)).alias(c) for c in columns]\n",
    "    null_counts_row = df.select(*exprs).collect()[0].asDict()\n",
    "\n",
    "    rows = []\n",
    "    for c in columns:\n",
    "        nulls = int(null_counts_row[c])\n",
    "        present = total_rows - nulls\n",
    "        null_pct = (nulls / total_rows * 100.0) if total_rows else 0.0\n",
    "        rows.append({\"column\": c, \"present_count\": present, \"null_count\": nulls, \"null_pct\": round(null_pct, 4)})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"null_pct\", ascending=False, ignore_index=True)\n",
    "\n",
    "def missing_matrix(df, columns, max_rows = 5000, include_empty_str = True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns given\")\n",
    "\n",
    "    with_id = df.select(F.monotonically_increasing_id().alias(\"_rid\"), *columns).orderBy(\"_rid\").limit(max_rows)\n",
    "\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "    miss_cols = [is_missing_expr(c, schema_map[c], include_empty_str).alias(c) for c in columns]\n",
    "\n",
    "    bin_df = with_id.select(\"_rid\", *miss_cols).orderBy(\"_rid\").drop(\"_rid\")\n",
    "    pdf = bin_df.toPandas()\n",
    "    return pdf\n",
    "\n",
    "def visualize_missing(df, columns, max_rows = 2000, include_empty_str = True):\n",
    "    \"\"\"\n",
    "    Visualisation that gives basic understanding of amount of pressent and missing data (with % of missing data), plots heatmap showing where missing data is (displays only columns where there are missing values) and bar chart of null % in the data \n",
    "    \"\"\"\n",
    "    summary_pdf = missing_summary(df, columns=columns, include_empty_str=include_empty_str)\n",
    "    # Keeping columns that have some missing values\n",
    "    cols_with_nulls = summary_pdf.loc[summary_pdf[\"null_count\"] > 0, \"column\"].tolist()\n",
    "\n",
    "    if not cols_with_nulls:\n",
    "        print(\"No missing values detected\")\n",
    "        display(spark.createDataFrame(summary_pdf))\n",
    "        return summary_pdf\n",
    "\n",
    "    # Ploting heatmap: green - data is present, red - missing data (null value)\n",
    "    mat_pdf = missing_matrix(df, columns=cols_with_nulls, max_rows=max_rows, include_empty_str=include_empty_str)\n",
    "    if not mat_pdf.empty:\n",
    "        mat = mat_pdf.values.astype(np.int8)\n",
    "        fig1, ax1 = plt.subplots(figsize=(12, 10))\n",
    "        cmap = ListedColormap([\"#1F449C\", \"#F05039\"])\n",
    "        norm = BoundaryNorm([-0.5, 0.5, 1.5], cmap.N)\n",
    "        ax1.imshow(mat, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "        ax1.set_title(f\"Missingness heatmap\")\n",
    "        ax1.set_xlabel(\"Columns\")\n",
    "        ax1.set_ylabel(\"Row number\")\n",
    "        ax1.set_xticks(range(len(mat_pdf.columns)))\n",
    "        ax1.set_xticklabels(mat_pdf.columns, rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Ploting bar charts with null % per column\n",
    "    filtered_summary = summary_pdf[summary_pdf[\"column\"].isin(cols_with_nulls)]\n",
    "    if not filtered_summary.empty:\n",
    "        fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
    "        ax2.bar(filtered_summary[\"column\"], filtered_summary[\"null_pct\"])\n",
    "        ax2.set_title(\"Null % per column\")\n",
    "        ax2.set_xlabel(\"Column\")\n",
    "        ax2.set_ylabel(\"Null percentage (%)\")\n",
    "        ax2.set_xticklabels(filtered_summary[\"column\"], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    display(spark.createDataFrame(summary_pdf))\n",
    "    return summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c6e535f-8fc2-4ecf-920e-656f6c96a478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = visualize_missing(df_paris, columns=None, max_rows=15000, include_empty_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72f71d9-0220-4bc3-a7ff-7529460ab9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see now, there are 2 columns have no data at all. They are **, calendar_updated, neighbourhood_group_cleansed**\n",
    "\n",
    "I our case, there is not much we can do about this missing information and it can pe overlooked in a general research.\n",
    "Still, let'stake a look at other partialy missing data. From our previous researches on similar datasets we know that column bathrooms is connected to bathrooms_text, so we can fill in the bathrooms column by extracting information from bathrooms_text. \n",
    "\n",
    "Column bathrooms_text is a pretty standardised column where not many cases can be expected: \n",
    "* Number word case: most common, like 1 bath\n",
    "* Null case: empty cell, so we will put null to the bathrooms as well\n",
    "* Half bath case: sometimes we don't get a number but only the \"half bath\" words that need to be taken into account\n",
    "\n",
    "So, in the future we need to transform the bathrooms column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39dfd9c7-b9fc-47bb-bb2a-b90c6d727df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's look into other missing values and try to find any corelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac747f47-864d-4e55-923b-7f90fda40862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def missing_rate_expr(col: str):\n",
    "    return (F.count(F.when(F.col(col).isNull(), 1)) / F.count(F.lit(1))).alias(col)\n",
    "\n",
    "def missing_indicator(col: str):\n",
    "    return F.when(F.col(col).isNull(), 1).otherwise(0).alias(col + \"_miss\")\n",
    "\n",
    "def to_pandas(df, limit=None):\n",
    "    return (df.limit(limit) if isinstance(limit, int) else df).toPandas()\n",
    "\n",
    "SAMPLE_FRAC = 0.20  \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fcc9da-3a24-4874-8d81-5779d34adf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = df_paris.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_paris.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "cols_with_nulls = [c for c, n in null_counts.items() if n and n > 0]\n",
    "\n",
    "miss_df = df_paris.select([missing_indicator(c) for c in cols_with_nulls])\n",
    "miss_df_sample = miss_df.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n",
    "\n",
    "pdf = miss_df_sample.toPandas()\n",
    "\n",
    "corr = pdf.corr(numeric_only=True)\n",
    "corr.index.name = \"col\"\n",
    "corr.columns.name = \"col\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e21f10-ffa0-4102-ac91-e27c37cffadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "miss_rates = miss_df_sample.agg(*[F.mean(c).alias(c) for c in miss_df_sample.columns]).collect()[0].asDict()\n",
    "top_cols = [k for k,_ in sorted(miss_rates.items(), key=lambda kv: kv[1], reverse=True)[:N]]\n",
    "\n",
    "corr_top = corr.loc[top_cols, top_cols].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "im = plt.imshow(corr_top, aspect='auto')\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(top_cols)), labels=top_cols, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(top_cols)), labels=top_cols)\n",
    "plt.title(\"Co-missingness Correlation (Top N columns)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5abbbf-f6d0-400d-b8b7-7afb404c5fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see from the plot, all missing review data correlate with each other, as well as neighbourhood information, and info about the host (name, start date, response time, profile, etc). and correlation between missing prices, beds, and bathrooms\n",
    "\n",
    "As for reviews, the data might be missing due to the fact that the listing has yet to have reviews. \n",
    "\n",
    "As for neighbourhood information, it was probably not filled in by the host\n",
    "\n",
    "In case of host data correlation, it is also clearly not a random loss. There are several possible reasons why this data might be missing:\n",
    "* Host account was deleted or anonymised ed \n",
    "* Airbnb scrapper masked identifying information for privacy\n",
    "* Listing was terminated\n",
    "\n",
    "Now, let's take a look at correlation between price, beds and bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b7b959-7f6d-4a99-8065-d04b86e1c25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col1 = \"price\"\n",
    "col2 = \"beds\"\n",
    "col3= \"bathrooms\"\n",
    "col4 = \"bathrooms_text\"\n",
    "col5 = \"last_review\"\n",
    "col6 = \"license\"\n",
    "\n",
    "\n",
    "df_nulls = df_paris.filter(F.col(col1).isNull()).select(col1, col2, col3, col4, col5, col6)\n",
    "\n",
    "display(df_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb40d198-ecf4-4c3e-8497-8ca22d779a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From this we can see that most data that has missing beds and prices have either no licences or last review was made years ago. From this we can make an assumption that when the listing is no longer active, we cannot see data about rooms and prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d15a5bc-8fb7-4e71-89d2-0df0cd4cb1b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "However, there are at least 2 columns that can have null values that will not be displayed here \n",
    "\n",
    "They are: amenities and host_verifications\n",
    "\n",
    "From looking at the columns it can be seen that there might be no information in them, but the column will have jutst [] instead of null, which also affects our research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e40d98-807e-4e38-a103-8312d120413e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_check = [\"host_verifications\", \"amenities\"]\n",
    "\n",
    "conditions = {\n",
    "    c: (\n",
    "        F.col(c).isNull() |\n",
    "        (F.trim(F.col(c)) == \"\") |\n",
    "        (F.trim(F.col(c)) == \"[]\") |\n",
    "        (F.trim(F.col(c)) == \"['']\") |\n",
    "        (F.length(F.col(c)) <= 4)\n",
    "    )\n",
    "    for c in cols_to_check\n",
    "}\n",
    "\n",
    "combined_condition = None\n",
    "for cond in conditions.values():\n",
    "    combined_condition = cond if combined_condition is None else (combined_condition | cond)\n",
    "\n",
    "filtered_df = df_paris.filter(combined_condition).select(*cols_to_check)\n",
    "\n",
    "display(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47294737-cddc-45ab-9910-6c81640bd2f8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762291291738}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_check = [\"host_verifications\", \"amenities\"]\n",
    "\n",
    "conditions = {\n",
    "    c: (\n",
    "        F.col(c).isNull() |\n",
    "        (F.trim(F.col(c)) == \"\") |\n",
    "        (F.trim(F.col(c)) == \"[]\") |\n",
    "        (F.trim(F.col(c)) == \"['']\") |\n",
    "        (F.length(F.col(c)) <= 4)\n",
    "    )\n",
    "    for c in cols_to_check\n",
    "}\n",
    "\n",
    "summary_df = df_paris.agg(\n",
    "    *[\n",
    "        F.sum(F.when(conditions[c], 1).otherwise(0)).alias(f\"{c}_empty_count\")\n",
    "        for c in cols_to_check\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30e5169-141d-46c7-92cc-81f35d3ca423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, there are some missing values although they were not visible before, and not just empty list, sometimes it's also word None, so we will have to clean them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b35475e-69ce-45c8-8e78-b88acf6f33e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Venice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99914585-bbc2-4d3a-9740-1590d558ba59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = visualize_missing(df_venice, columns=None, max_rows=15000, include_empty_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc02c01-dfc4-41e0-902f-ad29407e030c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, Venice dataset has a lot fewer fully missing columns, and in our case, fully missing columns, and in our case the only one with no info is calendar_updated, which can be overlooked.\n",
    "\n",
    "Here bathrooms column has filled information, but upon looking into it, we found out that some values are missing, although the bathrooms_text column is filled for them. That's why in the future we will need to work with this column in a similar way that was proposed for Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01fbac23-3a11-4a68-9ed4-5821ba2d7bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "col1 = \"bathrooms\"\n",
    "col2 = \"bathrooms_text\"\n",
    "\n",
    "df_nulls = df_venice.filter(F.col(col1).isNull()).select(col1, col2)\n",
    "\n",
    "display(df_nulls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76725abd-79c3-4f58-bec1-81bce7b24736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = df_venice.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_venice.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "cols_with_nulls = [c for c, n in null_counts.items() if n and n > 0]\n",
    "\n",
    "miss_df = df_venice.select([missing_indicator(c) for c in cols_with_nulls])\n",
    "miss_df_sample = miss_df.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n",
    "\n",
    "pdf = miss_df_sample.toPandas()\n",
    "\n",
    "corr = pdf.corr(numeric_only=True)\n",
    "corr.index.name = \"col\"\n",
    "corr.columns.name = \"col\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c468e51-403c-4c81-bcf7-eb97d52fa021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "miss_rates = miss_df_sample.agg(*[F.mean(c).alias(c) for c in miss_df_sample.columns]).collect()[0].asDict()\n",
    "top_cols = [k for k,_ in sorted(miss_rates.items(), key=lambda kv: kv[1], reverse=True)[:N]]\n",
    "\n",
    "corr_top = corr.loc[top_cols, top_cols].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "im = plt.imshow(corr_top, aspect='auto')\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(top_cols)), labels=top_cols, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(top_cols)), labels=top_cols)\n",
    "plt.title(\"Co-missingness Correlation (Top N columns)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146bc409-38c9-4aa9-a3cc-dc1ad976a798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The correlation case is also quite similar to the Paris case, reasoning for which we wrote before, but apart from them, we also see a correlation between missing values for price, revenue, beds and bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0513a17-7342-417f-b454-f53979233375",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762080766298}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col1 = \"price\"\n",
    "col2 = \"beds\"\n",
    "col3= \"bathrooms\"\n",
    "col4 = \"bathrooms_text\"\n",
    "col5 = \"last_review\"\n",
    "col6 = \"license\"\n",
    "\n",
    "\n",
    "df_nulls = df_venice.filter(F.col(col1).isNull()).select(col1, col2, col3, col4, col5, col6)\n",
    "\n",
    "display(df_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f4b9f2-c48b-48b8-9649-f61362db582c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Looking into the filterd table, we can see that batroom infromation can be filled, so the correlation will dissaper once its done, but for others we can make an assumption that the most common reason why all this data is missing is because the accomodation is currently unavilable or was terminated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14514487-d887-45f3-b00f-a0e6f81fee22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols_to_check = [\"host_verifications\", \"amenities\"]\n",
    "\n",
    "conditions = {\n",
    "    c: (\n",
    "        F.col(c).isNull() |\n",
    "        (F.trim(F.col(c)) == \"\") |\n",
    "        (F.trim(F.col(c)) == \"[]\") |\n",
    "        (F.trim(F.col(c)) == \"['']\") |\n",
    "        (F.length(F.col(c)) <= 4)\n",
    "    )\n",
    "    for c in cols_to_check\n",
    "}\n",
    "\n",
    "summary_df = df_venice.agg(\n",
    "    *[\n",
    "        F.sum(F.when(conditions[c], 1).otherwise(0)).alias(f\"{c}_empty_count\")\n",
    "        for c in cols_to_check\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02410d60-34b6-4d08-961e-9313ab8306de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, in this dataset we also have some hidden missing data in host verification and amenities, so we will have to clean it in the future"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6006278670822208,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Exploring_raw_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
