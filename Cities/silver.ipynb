{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eec8a3-8a05-4e59-aa19-5525fae92299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4872c5da-365a-4d9c-84c3-3b24e0968f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(\"workspace.airbnb_bronze.listings_raw\")\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf500ec8-89a5-4245-8748-62765d15030d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"  \n",
    "BRONZE_TABLE = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "dbutils.widgets.text(\"city\", \"Paris\")\n",
    "city_name = dbutils.widgets.get(\"city\")\n",
    "print(f\"[silver] Transform for city={city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5637367a-66f5-47e3-aa71-dae711b5418a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(\"workspace.airbnb_silver.listings\")\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    "         .orderBy(\"city\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04793c71-e79c-4ef9-8941-efb4a6a3b04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "\n",
    "city_name = \"Venice\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 0: Load bronze data for this city\n",
    "# ---------------------------------------------------------\n",
    "bronze_city = (\n",
    "    spark.table(BRONZE_TABLE)\n",
    "         .where(F.col(\"city\") == city_name)\n",
    ")\n",
    "\n",
    "print(\"Step 0: raw Paris rows in bronze\")\n",
    "print(bronze_city.count())\n",
    "\n",
    "print(\"Columns in Paris bronze dataset:\")\n",
    "print(bronze_city.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ§¼ Step 0.1: Clean empty list-like strings\n",
    "# ---------------------------------------------------------\n",
    "def clean_empty_list_strings(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.col(c).isNull() |\n",
    "                    (F.trim(F.col(c)) == \"\") |\n",
    "                    (F.trim(F.col(c)) == \"[]\") |\n",
    "                    (F.trim(F.col(c)) == \"['']\") |\n",
    "                    (F.length(F.col(c)) <= 4),\n",
    "                    F.lit(None)\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "    return df\n",
    "\n",
    "cols_to_clean = [\"host_verifications\", \"amenities\"]\n",
    "bronze_city = clean_empty_list_strings(bronze_city, cols_to_clean)\n",
    "print(f\"[silver] Cleaned empty placeholders in {cols_to_clean}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 1: Identify dynamic columns (price, lat, lon)\n",
    "# ---------------------------------------------------------\n",
    "price_candidates = [c for c in [\"price\",\"price_x\",\"price_y\",\"cost\",\"nightly_price\"] if c in bronze_city.columns]\n",
    "lat_candidates   = [c for c in [\"latitude\",\"lat\",\"Latitude\",\"geo_lat\"] if c in bronze_city.columns]\n",
    "lon_candidates   = [c for c in [\"longitude\",\"lon\",\"Longitude\",\"lng\",\"geo_lon\"] if c in bronze_city.columns]\n",
    "\n",
    "print(\"Price candidates:\", price_candidates)\n",
    "print(\"Lat candidates:\", lat_candidates)\n",
    "print(\"Lon candidates:\", lon_candidates)\n",
    "\n",
    "price_col = price_candidates[0] if price_candidates else None\n",
    "lat_col   = lat_candidates[0]   if lat_candidates else None\n",
    "lon_col   = lon_candidates[0]   if lon_candidates else None\n",
    "\n",
    "num_regex = r'(\\d+(\\.\\d+)?)'\n",
    "bronze_city_typed = bronze_city\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Cast and clean numeric / text fields\n",
    "# ---------------------------------------------------------\n",
    "# bathrooms_clean\n",
    "bronze_city_typed = bronze_city_typed.withColumn(\n",
    "    \"bathrooms_clean\",\n",
    "    F.when(\n",
    "        F.col(\"bathrooms_text\").rlike(\"(?i)^\\\\s*half\"), F.lit(0.5)\n",
    "    ).when(\n",
    "        F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"),\n",
    "        F.lit(None).cast(\"double\")\n",
    "    ).when(\n",
    "        F.col(\"bathrooms_text\").rlike(num_regex),\n",
    "        F.regexp_extract(\"bathrooms_text\", num_regex, 1).cast(\"double\")\n",
    "    ).otherwise(F.lit(None).cast(\"double\"))\n",
    ")\n",
    "\n",
    "# price_clean (robust euro/commas/space parsing)\n",
    "if price_col is not None:\n",
    "    # Step 1: replace commas with dots (e.g. \"120,50 â‚¬\" -> \"120.50 â‚¬\")\n",
    "    tmp_price = F.regexp_replace(F.col(price_col), \",\", \".\")\n",
    "    # Step 2: remove everything except digits and dot (\"â‚¬1 234.50 EUR\" -> \"1234.50\")\n",
    "    tmp_price = F.regexp_replace(tmp_price, r\"[^0-9\\.]\", \"\")\n",
    "    # Step 3: extract first numeric token\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_number_str\",\n",
    "        F.regexp_extract(tmp_price, r\"(\\d+(\\.\\d+)?)\", 1)\n",
    "    )\n",
    "    # Step 4: cast to double\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_clean\",\n",
    "        F.col(\"price_number_str\").cast(\"double\")\n",
    "    )\n",
    "else:\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_clean\", F.lit(None).cast(\"double\")\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Date and numeric conversions\n",
    "# ---------------------------------------------------------\n",
    "bronze_city_typed = (\n",
    "    bronze_city_typed\n",
    "    .withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "    .withColumn(\"host_since_dt\",   F.to_date(\"host_since\"))\n",
    "    .withColumn(\"availability_30_int\",  F.col(\"availability_30\").cast(\"int\"))\n",
    "    .withColumn(\"availability_365_int\", F.col(\"availability_365\").cast(\"int\"))\n",
    "    .withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "print(\"Step 1: after type casting (bathrooms, price_clean, dates):\")\n",
    "print(bronze_city_typed.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Keep only the latest snapshot per listing_id\n",
    "# ---------------------------------------------------------\n",
    "w_latest = Window.partitionBy(\"listing_id\").orderBy(\n",
    "    F.col(\"last_scraped_dt\").desc_nulls_last(),\n",
    "    F.col(\"ingestion_timestamp\").desc_nulls_last()\n",
    ")\n",
    "\n",
    "silver_latest_city = (\n",
    "    bronze_city_typed\n",
    "    .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(\"Step 2: after keeping latest snapshot per listing_id:\")\n",
    "print(silver_latest_city.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 5: Apply quality filters\n",
    "# ---------------------------------------------------------\n",
    "if lat_col is not None:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lat_tmp\", F.col(lat_col).cast(\"double\"))\n",
    "else:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lat_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "if lon_col is not None:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lon_tmp\", F.col(lon_col).cast(\"double\"))\n",
    "else:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lon_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "silver_filtered_preview = (\n",
    "    silver_latest_city\n",
    "    .where(F.col(\"price_clean\").isNotNull())\n",
    "    .where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "    .where(F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"Step 3: after applying quality filters we use in silver:\")\n",
    "print(silver_filtered_preview.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 6: Show sample data before/after filters\n",
    "# ---------------------------------------------------------\n",
    "print(\"Example rows that PASSED filters:\")\n",
    "display(\n",
    "    silver_filtered_preview.select(\n",
    "        \"listing_id\", \"price_clean\", \"lat_tmp\", \"lon_tmp\", \"last_scraped_dt\"\n",
    "    ).limit(20)\n",
    ")\n",
    "\n",
    "print(\"Example rows BEFORE filters (to inspect raw columns):\")\n",
    "display(\n",
    "    silver_latest_city.select(\n",
    "        \"listing_id\",\n",
    "        \"price_clean\",\n",
    "        \"bathrooms_text\",\n",
    "        \"bathrooms_clean\",\n",
    "        lat_col if lat_col else F.lit(None).alias(\"lat_col_missing\"),\n",
    "        lon_col if lon_col else F.lit(None).alias(\"lon_col_missing\"),\n",
    "        \"last_scraped_dt\",\n",
    "        \"ingestion_timestamp\"\n",
    "    ).limit(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80b5a50-0a2d-4031-ad78-09010e07a3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\" \n",
    "BRONZE_TABLE          = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.airbnb_silver\")\n",
    "\n",
    "# ---- city param ----\n",
    "dbutils.widgets.text(\"city\", \"Paris\")\n",
    "city_name = dbutils.widgets.get(\"city\")\n",
    "print(f\"[silver] Transform for city={city_name}\")\n",
    "\n",
    "# ---- load bronze for that city ----\n",
    "bronze_city = (\n",
    "    spark.table(BRONZE_TABLE)\n",
    "         .where(F.col(\"city\") == city_name)\n",
    ")\n",
    "\n",
    "print(f\"[silver] bronze rows for {city_name}: {bronze_city.count()}\")\n",
    "\n",
    "# === INSERTED: clean empty list-like strings ('[]', \"['']\", blanks) ===\n",
    "def clean_empty_list_strings(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.col(c).isNull() |\n",
    "                    (F.trim(F.col(c)) == \"\") |\n",
    "                    (F.trim(F.col(c)) == \"[]\") |\n",
    "                    (F.trim(F.col(c)) == \"['']\") |\n",
    "                    (F.length(F.col(c)) <= 4),\n",
    "                    F.lit(None)\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "    return df\n",
    "\n",
    "\n",
    "cols_to_clean = [\"host_verifications\", \"amenities\"]\n",
    "bronze_city = clean_empty_list_strings(bronze_city, cols_to_clean)\n",
    "print(f\"[silver] Cleaned empty placeholders in: {cols_to_clean}\")\n",
    "\n",
    "price_candidates = [c for c in [\"price\",\"price_x\",\"price_y\",\"cost\",\"nightly_price\"] if c in bronze_city.columns]\n",
    "lat_candidates   = [c for c in [\"latitude\",\"lat\",\"Latitude\",\"geo_lat\"]               if c in bronze_city.columns]\n",
    "lon_candidates   = [c for c in [\"longitude\",\"lon\",\"Longitude\",\"lng\",\"geo_lon\"]       if c in bronze_city.columns]\n",
    "\n",
    "price_col = price_candidates[0] if price_candidates else None\n",
    "lat_col   = lat_candidates[0]   if lat_candidates else None\n",
    "lon_col   = lon_candidates[0]   if lon_candidates else None\n",
    "\n",
    "print(\"[silver] using columns:\",\n",
    "      \"price_col =\", price_col,\n",
    "      \"lat_col =\", lat_col,\n",
    "      \"lon_col =\", lon_col)\n",
    "\n",
    "num_regex_bath = r'(\\d+(\\.\\d+)?)' \n",
    "\n",
    "df_typed = bronze_city\n",
    "\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"bathrooms_clean\",\n",
    "    F.when(\n",
    "        F.col(\"bathrooms_text\").rlike(\"(?i)^\\\\s*half\"),\n",
    "        F.lit(0.5)\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"),\n",
    "        F.lit(None).cast(\"double\")\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"bathrooms_text\").rlike(num_regex_bath),\n",
    "        F.regexp_extract(\"bathrooms_text\", num_regex_bath, 1).cast(\"double\")\n",
    "    )\n",
    "    .otherwise(F.lit(None).cast(\"double\"))\n",
    ")\n",
    "\n",
    "if price_col is not None:\n",
    "    # normalize commas to dots\n",
    "    tmp_price = F.regexp_replace(F.col(price_col), \",\", \".\")\n",
    "    # remove currency symbols/letters/spaces, keep digits and dots\n",
    "    tmp_price = F.regexp_replace(tmp_price, r\"[^0-9\\.]\", \"\")\n",
    "    # grab first numeric token\n",
    "    df_typed = df_typed.withColumn(\n",
    "        \"price_number_str\",\n",
    "        F.regexp_extract(tmp_price, r\"(\\d+(\\.\\d+)?)\", 1)\n",
    "    )\n",
    "    # cast final numeric\n",
    "    df_typed = df_typed.withColumn(\n",
    "        \"price_clean\",\n",
    "        F.col(\"price_number_str\").cast(\"double\")\n",
    "    )\n",
    "else:\n",
    "    df_typed = df_typed.withColumn(\n",
    "        \"price_clean\", F.lit(None).cast(\"double\")\n",
    "    )\n",
    "\n",
    "df_typed = (\n",
    "    df_typed\n",
    "    .withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "    .withColumn(\"host_since_dt\",   F.to_date(\"host_since\"))\n",
    "    .withColumn(\"availability_30_int\",  F.col(\"availability_30\").cast(\"int\"))\n",
    "    .withColumn(\"availability_365_int\", F.col(\"availability_365\").cast(\"int\"))\n",
    "    .withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "w_latest = Window.partitionBy(\"listing_id\").orderBy(\n",
    "    F.col(\"last_scraped_dt\").desc_nulls_last(),\n",
    "    F.col(\"ingestion_timestamp\").desc_nulls_last()\n",
    ")\n",
    "\n",
    "latest_only = (\n",
    "    df_typed\n",
    "    .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "count_latest = latest_only.count()\n",
    "print(f\"[silver] {city_name}: rows after latest snapshot per listing_id = {count_latest}\")\n",
    "\n",
    "if lat_col is not None:\n",
    "    latest_only = latest_only.withColumn(\"lat_tmp\", F.col(lat_col).cast(\"double\"))\n",
    "else:\n",
    "    latest_only = latest_only.withColumn(\"lat_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "if lon_col is not None:\n",
    "    latest_only = latest_only.withColumn(\"lon_tmp\", F.col(lon_col).cast(\"double\"))\n",
    "else:\n",
    "    latest_only = latest_only.withColumn(\"lon_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "non_null_price_cnt = latest_only.where(F.col(\"price_clean\").isNotNull()).count()\n",
    "print(f\"[silver] {city_name}: listings with non-null price_clean = {non_null_price_cnt}\")\n",
    "\n",
    "if non_null_price_cnt > 0:\n",
    "    filtered_stage_a = (\n",
    "        latest_only\n",
    "        .where(F.col(\"price_clean\").isNotNull())\n",
    "        .where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "    )\n",
    "else:\n",
    "    print(f\"[silver] {city_name}: WARNING price parsing failed for entire city, skipping price filter.\")\n",
    "    filtered_stage_a = latest_only\n",
    "\n",
    "count_stage_a = filtered_stage_a.count()\n",
    "print(f\"[silver] {city_name}: rows after price filter logic = {count_stage_a}\")\n",
    "\n",
    "non_null_geo_cnt = filtered_stage_a.where(\n",
    "    F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull()\n",
    ").count()\n",
    "print(f\"[silver] {city_name}: listings with valid lat/lon = {non_null_geo_cnt}\")\n",
    "\n",
    "if non_null_geo_cnt > 0:\n",
    "    filtered_stage_b = (\n",
    "        filtered_stage_a\n",
    "        .where(F.col(\"lat_tmp\").isNotNull())\n",
    "        .where(F.col(\"lon_tmp\").isNotNull())\n",
    "    )\n",
    "    count_stage_b = filtered_stage_b.count()\n",
    "    print(f\"[silver] {city_name}: rows after geo filter logic = {count_stage_b}\")\n",
    "    final_filtered = filtered_stage_b if count_stage_b > 0 else filtered_stage_a\n",
    "else:\n",
    "    print(f\"[silver] {city_name}: WARNING no usable lat/lon, skipping geo filter.\")\n",
    "    final_filtered = filtered_stage_a\n",
    "\n",
    "final_count = final_filtered.count()\n",
    "print(f\"[silver] {city_name}: FINAL rows to merge into silver = {final_count}\")\n",
    "\n",
    "final_to_write = final_filtered.drop(\"lat_tmp\", \"lon_tmp\", \"price_number_str\")\n",
    "\n",
    "if not spark.catalog.tableExists(SILVER_LISTINGS_TABLE):\n",
    "    (\n",
    "        final_to_write\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"city\")\n",
    "        .saveAsTable(SILVER_LISTINGS_TABLE)\n",
    "    )\n",
    "    print(f\"[silver] Created {SILVER_LISTINGS_TABLE} with first batch ({city_name})\")\n",
    "else:\n",
    "    final_to_write.createOrReplaceTempView(\"silver_new_batch\")\n",
    "\n",
    "    cols = final_to_write.columns\n",
    "    merge_condition = \"t.listing_id = s.listing_id AND t.city = s.city\"\n",
    "\n",
    "    set_updates = \",\\n\".join([f\"t.{c} = s.{c}\" for c in cols])\n",
    "    insert_cols = \", \".join(cols)\n",
    "    insert_vals = \", \".join([f\"s.{c}\" for c in cols])\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {SILVER_LISTINGS_TABLE} t\n",
    "        USING silver_new_batch s\n",
    "        ON {merge_condition}\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        {set_updates}\n",
    "        WHEN NOT MATCHED THEN INSERT ({insert_cols})\n",
    "        VALUES ({insert_vals})\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"[silver] Merged {city_name} rows into {SILVER_LISTINGS_TABLE}\")\n",
    "\n",
    "# --- build / upsert hosts ---\n",
    "silver_hosts_city = (\n",
    "    final_to_write\n",
    "    .filter(F.col(\"host_id\").isNotNull())\n",
    "    .groupBy(\"city\", \"host_id\")\n",
    "    .agg(\n",
    "        F.min(\"host_since_dt\").alias(\"host_since_dt\"),\n",
    "        F.countDistinct(\"listing_id\").alias(\"listings_count\"),\n",
    "        F.first(\"host_name\").alias(\"host_name\"),\n",
    "        F.first(\"host_is_superhost\").alias(\"host_is_superhost\")\n",
    "    )\n",
    "    .withColumn(\"host_since_year\", F.year(\"host_since_dt\"))\n",
    ")\n",
    "\n",
    "if not spark.catalog.tableExists(SILVER_HOSTS_TABLE):\n",
    "    (\n",
    "        silver_hosts_city\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"city\")\n",
    "        .saveAsTable(SILVER_HOSTS_TABLE)\n",
    "    )\n",
    "    print(f\"[silver] Created {SILVER_HOSTS_TABLE} with first batch ({city_name})\")\n",
    "else:\n",
    "    silver_hosts_city.createOrReplaceTempView(\"hosts_new_batch\")\n",
    "\n",
    "    hcols = silver_hosts_city.columns\n",
    "    merge_condition_hosts = \"t.host_id = s.host_id AND t.city = s.city\"\n",
    "\n",
    "    set_updates_hosts = \",\\n\".join([f\"t.{c} = s.{c}\" for c in hcols])\n",
    "    insert_cols_hosts = \", \".join(hcols)\n",
    "    insert_vals_hosts = \", \".join([f\"s.{c}\" for c in hcols])\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {SILVER_HOSTS_TABLE} t\n",
    "        USING hosts_new_batch s\n",
    "        ON {merge_condition_hosts}\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        {set_updates_hosts}\n",
    "        WHEN NOT MATCHED THEN INSERT ({insert_cols_hosts})\n",
    "        VALUES ({insert_vals_hosts})\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"[silver] Merged {city_name} host rows into {SILVER_HOSTS_TABLE}\")\n",
    "\n",
    "# --- sanity check final silver ---\n",
    "display(\n",
    "    spark.table(SILVER_LISTINGS_TABLE)\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    "         .orderBy(\"city\")\n",
    ")\n",
    "\n",
    "display(\n",
    "    spark.table(SILVER_HOSTS_TABLE)\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    "         .orderBy(\"city\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70af5cd7-dca4-45b5-b53e-c81e06ba568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.table(\"workspace.airbnb_silver.listings\")\n",
    "\n",
    "display(\n",
    "    silver.groupBy(\"city\").agg(\n",
    "        F.sum(F.col(\"host_verifications\").isNull().cast(\"int\")).alias(\"empty_host_verifications\"),\n",
    "        F.sum(F.col(\"amenities\").isNull().cast(\"int\")).alias(\"empty_amenities\")\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {
    "city": {
     "currentValue": "Paris",
     "nuid": "a656cfdc-ed64-49c9-988e-ac2e3c0be910",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
