{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eec8a3-8a05-4e59-aa19-5525fae92299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4872c5da-365a-4d9c-84c3-3b24e0968f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(\"workspace.airbnb_bronze.listings_raw\")\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf500ec8-89a5-4245-8748-62765d15030d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"  \n",
    "BRONZE_TABLE = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "dbutils.widgets.text(\"city\", \"Paris\")\n",
    "city_name = dbutils.widgets.get(\"city\")\n",
    "print(f\"[silver] Transform for city={city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5637367a-66f5-47e3-aa71-dae711b5418a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(\"workspace.airbnb_silver.listings\")\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    "         .orderBy(\"city\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04793c71-e79c-4ef9-8941-efb4a6a3b04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "\n",
    "city_name = \"Venice\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 0: Load bronze data for this city\n",
    "# ---------------------------------------------------------\n",
    "bronze_city = (\n",
    "    spark.table(BRONZE_TABLE)\n",
    "         .where(F.col(\"city\") == city_name)\n",
    ")\n",
    "\n",
    "print(\"Step 0: raw Paris rows in bronze\")\n",
    "print(bronze_city.count())\n",
    "\n",
    "print(\"Columns in Paris bronze dataset:\")\n",
    "print(bronze_city.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ§¼ Step 0.1: Clean empty list-like strings\n",
    "# ---------------------------------------------------------\n",
    "def clean_empty_list_strings(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.col(c).isNull() |\n",
    "                    (F.trim(F.col(c)) == \"\") |\n",
    "                    (F.trim(F.col(c)) == \"[]\") |\n",
    "                    (F.trim(F.col(c)) == \"['']\") |\n",
    "                    (F.length(F.col(c)) <= 4),\n",
    "                    F.lit(None)\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "    return df\n",
    "\n",
    "cols_to_clean = [\"host_verifications\", \"amenities\"]\n",
    "bronze_city = clean_empty_list_strings(bronze_city, cols_to_clean)\n",
    "print(f\"[silver] Cleaned empty placeholders in {cols_to_clean}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 1: Identify dynamic columns (price, lat, lon)\n",
    "# ---------------------------------------------------------\n",
    "price_candidates = [c for c in [\"price\",\"price_x\",\"price_y\",\"cost\",\"nightly_price\"] if c in bronze_city.columns]\n",
    "lat_candidates   = [c for c in [\"latitude\",\"lat\",\"Latitude\",\"geo_lat\"] if c in bronze_city.columns]\n",
    "lon_candidates   = [c for c in [\"longitude\",\"lon\",\"Longitude\",\"lng\",\"geo_lon\"] if c in bronze_city.columns]\n",
    "\n",
    "print(\"Price candidates:\", price_candidates)\n",
    "print(\"Lat candidates:\", lat_candidates)\n",
    "print(\"Lon candidates:\", lon_candidates)\n",
    "\n",
    "price_col = price_candidates[0] if price_candidates else None\n",
    "lat_col   = lat_candidates[0]   if lat_candidates else None\n",
    "lon_col   = lon_candidates[0]   if lon_candidates else None\n",
    "\n",
    "num_regex = r'(\\d+(\\.\\d+)?)'\n",
    "bronze_city_typed = bronze_city\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Cast and clean numeric / text fields\n",
    "# ---------------------------------------------------------\n",
    "# bathrooms_clean\n",
    "bronze_city_typed = bronze_city_typed.withColumn(\n",
    "    \"bathrooms_clean\",\n",
    "    F.when(\n",
    "        F.col(\"bathrooms_text\").rlike(\"(?i)^\\\\s*half\"), F.lit(0.5)\n",
    "    ).when(\n",
    "        F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"),\n",
    "        F.lit(None).cast(\"double\")\n",
    "    ).when(\n",
    "        F.col(\"bathrooms_text\").rlike(num_regex),\n",
    "        F.regexp_extract(\"bathrooms_text\", num_regex, 1).cast(\"double\")\n",
    "    ).otherwise(F.lit(None).cast(\"double\"))\n",
    ")\n",
    "\n",
    "# price_clean (robust euro/commas/space parsing)\n",
    "if price_col is not None:\n",
    "    # Step 1: replace commas with dots (e.g. \"120,50 â‚¬\" -> \"120.50 â‚¬\")\n",
    "    tmp_price = F.regexp_replace(F.col(price_col), \",\", \".\")\n",
    "    # Step 2: remove everything except digits and dot (\"â‚¬1 234.50 EUR\" -> \"1234.50\")\n",
    "    tmp_price = F.regexp_replace(tmp_price, r\"[^0-9\\.]\", \"\")\n",
    "    # Step 3: extract first numeric token\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_number_str\",\n",
    "        F.regexp_extract(tmp_price, r\"(\\d+(\\.\\d+)?)\", 1)\n",
    "    )\n",
    "    # Step 4: cast to double\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_clean\",\n",
    "        F.col(\"price_number_str\").cast(\"double\")\n",
    "    )\n",
    "else:\n",
    "    bronze_city_typed = bronze_city_typed.withColumn(\n",
    "        \"price_clean\", F.lit(None).cast(\"double\")\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Date and numeric conversions\n",
    "# ---------------------------------------------------------\n",
    "bronze_city_typed = (\n",
    "    bronze_city_typed\n",
    "    .withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "    .withColumn(\"host_since_dt\",   F.to_date(\"host_since\"))\n",
    "    .withColumn(\"availability_30_int\",  F.col(\"availability_30\").cast(\"int\"))\n",
    "    .withColumn(\"availability_365_int\", F.col(\"availability_365\").cast(\"int\"))\n",
    "    .withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "print(\"Step 1: after type casting (bathrooms, price_clean, dates):\")\n",
    "print(bronze_city_typed.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Keep only the latest snapshot per listing_id\n",
    "# ---------------------------------------------------------\n",
    "w_latest = Window.partitionBy(\"id\").orderBy(\n",
    "    F.col(\"last_scraped_dt\").desc_nulls_last()\n",
    ")\n",
    "\n",
    "silver_latest_city = (\n",
    "    bronze_city_typed\n",
    "    .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "    .where(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(\"Step 2: after keeping latest snapshot per listing_id:\")\n",
    "print(silver_latest_city.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 5: Apply quality filters\n",
    "# ---------------------------------------------------------\n",
    "if lat_col is not None:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lat_tmp\", F.col(lat_col).cast(\"double\"))\n",
    "else:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lat_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "if lon_col is not None:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lon_tmp\", F.col(lon_col).cast(\"double\"))\n",
    "else:\n",
    "    silver_latest_city = silver_latest_city.withColumn(\"lon_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "silver_filtered_preview = (\n",
    "    silver_latest_city\n",
    "    .where(F.col(\"price_clean\").isNotNull())\n",
    "    .where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "    .where(F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"Step 3: after applying quality filters we use in silver:\")\n",
    "print(silver_filtered_preview.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 6: Show sample data before/after filters\n",
    "# ---------------------------------------------------------\n",
    "print(\"Example rows that PASSED filters:\")\n",
    "display(\n",
    "    silver_filtered_preview.select(\n",
    "        \"id\", \"price_clean\", \"lat_tmp\", \"lon_tmp\", \"last_scraped_dt\"\n",
    "    ).limit(20)\n",
    ")\n",
    "\n",
    "print(\"Example rows BEFORE filters (to inspect raw columns):\")\n",
    "display(\n",
    "    silver_latest_city.select(\n",
    "        \"id\",\n",
    "        \"price_clean\",\n",
    "        \"bathrooms_text\",\n",
    "        \"bathrooms_clean\",\n",
    "        lat_col if lat_col else F.lit(None).alias(\"lat_col_missing\"),\n",
    "        lon_col if lon_col else F.lit(None).alias(\"lon_col_missing\"),\n",
    "        \"last_scraped_dt\"    ).limit(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428a19f1-1fef-4abc-b979-633bcbf6b024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Build / Update SILVER (per-city) from BRONZE\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"\n",
    "BRONZE_TABLE          = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "\n",
    "\n",
    "# ---- City param (widget) ----\n",
    "try:\n",
    "    dbutils.widgets.get(\"city\")\n",
    "except Exception:\n",
    "    dbutils.widgets.text(\"city\", \"Paris\")\n",
    "city_name = dbutils.widgets.get(\"city\")\n",
    "print(f\"[silver] Transform for city = {city_name}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 0: Load bronze data for this city (and basic assertions)\n",
    "# ---------------------------------------------------------\n",
    "bronze_city = spark.table(BRONZE_TABLE).where(F.col(\"city\") == city_name)\n",
    "\n",
    "row_cnt = bronze_city.count()\n",
    "print(f\"[silver] bronze rows for {city_name}: {row_cnt}\")\n",
    "if row_cnt == 0:\n",
    "    raise ValueError(f\"No Bronze rows for city='{city_name}'. Check BRONZE_TABLE or city spelling.\")\n",
    "\n",
    "print(\"[silver] Columns in bronze:\", bronze_city.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 0.1: Defensive cleaning of empty list-like strings\n",
    "# ---------------------------------------------------------\n",
    "def clean_empty_list_strings(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.col(c).isNull() |\n",
    "                    (F.trim(F.col(c)) == \"\") |\n",
    "                    (F.trim(F.col(c)) == \"[]\") |\n",
    "                    (F.trim(F.col(c)) == \"['']\") |\n",
    "                    (F.length(F.col(c)) <= 4),\n",
    "                    F.lit(None)\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "    return df\n",
    "\n",
    "bronze_city = clean_empty_list_strings(bronze_city, [\"host_verifications\", \"amenities\"])\n",
    "print(f\"[silver] Cleaned placeholders in: ['host_verifications', 'amenities']\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 0.2: Create robust PK and ingestion timestamp if missing\n",
    "# ---------------------------------------------------------\n",
    "cset = set(bronze_city.columns)\n",
    "if \"listing_id\" in cset:\n",
    "    bronze_city = bronze_city.withColumn(\"pk_listing_id\", F.col(\"listing_id\").cast(\"string\"))\n",
    "elif \"id\" in cset:\n",
    "    bronze_city = bronze_city.withColumn(\"pk_listing_id\", F.col(\"id\").cast(\"string\"))\n",
    "elif \"listing_url\" in cset:\n",
    "    bronze_city = bronze_city.withColumn(\"pk_listing_id\", F.sha2(F.col(\"listing_url\"), 256))\n",
    "else:\n",
    "    raise ValueError(\"No suitable primary key available (need one of: listing_id, id, listing_url).\")\n",
    "\n",
    "if \"ingestion_timestamp\" not in cset:\n",
    "    bronze_city = bronze_city.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 1: Identify dynamic columns (price / lat / lon)\n",
    "# ---------------------------------------------------------\n",
    "price_candidates = [c for c in [\"price\",\"price_x\",\"price_y\",\"cost\",\"nightly_price\"] if c in bronze_city.columns]\n",
    "lat_candidates   = [c for c in [\"latitude\",\"lat\",\"Latitude\",\"geo_lat\"]               if c in bronze_city.columns]\n",
    "lon_candidates   = [c for c in [\"longitude\",\"lon\",\"Longitude\",\"lng\",\"geo_lon\"]       if c in bronze_city.columns]\n",
    "\n",
    "price_col = price_candidates[0] if price_candidates else None\n",
    "lat_col   = lat_candidates[0]   if lat_candidates else None\n",
    "lon_col   = lon_candidates[0]   if lon_candidates else None\n",
    "\n",
    "print(\"[silver] Using columns:\",\n",
    "      \"price_col =\", price_col, \"| lat_col =\", lat_col, \"| lon_col =\", lon_col)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Cast and clean numeric / text fields (guarded)\n",
    "# ---------------------------------------------------------\n",
    "num_regex = r'(\\d+(\\.\\d+)?)'\n",
    "df = bronze_city\n",
    "\n",
    "# bathrooms_clean (only if bathrooms_text exists)\n",
    "if \"bathrooms_text\" in df.columns:\n",
    "    df = df.withColumn(\n",
    "        \"bathrooms_clean\",\n",
    "        F.when(F.col(\"bathrooms_text\").rlike(\"(?i)^\\\\s*half\"), F.lit(0.5))\n",
    "         .when(F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"), F.lit(None).cast(\"double\"))\n",
    "         .when(F.col(\"bathrooms_text\").rlike(num_regex), F.regexp_extract(\"bathrooms_text\", num_regex, 1).cast(\"double\"))\n",
    "         .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "\n",
    "# price_clean (robust-ish for â‚¬, commas, spaces)\n",
    "if price_col is not None:\n",
    "    tmp_price = F.regexp_replace(F.col(price_col), \",\", \".\")\n",
    "    tmp_price = F.regexp_replace(tmp_price, r\"[^0-9\\.]\", \"\")\n",
    "    df = df.withColumn(\"price_number_str\", F.regexp_extract(tmp_price, num_regex, 1))\n",
    "    df = df.withColumn(\"price_clean\", F.col(\"price_number_str\").cast(\"double\"))\n",
    "else:\n",
    "    df = df.withColumn(\"price_clean\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "# dates & numerics (guarded)\n",
    "if \"last_scraped\" in df.columns:\n",
    "    df = df.withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "else:\n",
    "    df = df.withColumn(\"last_scraped_dt\", F.lit(None).cast(\"date\"))\n",
    "\n",
    "if \"host_since\" in df.columns:\n",
    "    df = df.withColumn(\"host_since_dt\", F.to_date(\"host_since\"))\n",
    "\n",
    "if \"availability_30\" in df.columns:\n",
    "    df = df.withColumn(\"availability_30_int\", F.col(\"availability_30\").cast(\"int\"))\n",
    "if \"availability_365\" in df.columns:\n",
    "    df = df.withColumn(\"availability_365_int\", F.col(\"availability_365\").cast(\"int\"))\n",
    "if \"review_scores_rating\" in df.columns:\n",
    "    df = df.withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    "\n",
    "# lat/lon clean\n",
    "if lat_col is not None:\n",
    "    df = df.withColumn(\"lat_tmp\", F.col(lat_col).cast(\"double\"))\n",
    "else:\n",
    "    df = df.withColumn(\"lat_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "if lon_col is not None:\n",
    "    df = df.withColumn(\"lon_tmp\", F.col(lon_col).cast(\"double\"))\n",
    "else:\n",
    "    df = df.withColumn(\"lon_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "print(\"[silver] After type casting:\", df.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Keep only the latest snapshot per listing within the city\n",
    "# ---------------------------------------------------------\n",
    "w_latest = Window.partitionBy(\"pk_listing_id\").orderBy(\n",
    "    F.col(\"last_scraped_dt\").desc_nulls_last(),\n",
    "    F.col(\"ingestion_timestamp\").desc_nulls_last()\n",
    ")\n",
    "\n",
    "latest_only = (\n",
    "    df.withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "      .where(F.col(\"rn\") == 1)\n",
    "      .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(f\"[silver] {city_name}: rows after latest-per-listing =\", latest_only.count())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Apply quality filters with safety checks\n",
    "# ---------------------------------------------------------\n",
    "non_null_price_cnt = latest_only.where(F.col(\"price_clean\").isNotNull()).count()\n",
    "print(f\"[silver] {city_name}: listings with non-null price_clean = {non_null_price_cnt}\")\n",
    "\n",
    "if non_null_price_cnt > 0:\n",
    "    filtered_stage_a = (\n",
    "        latest_only\n",
    "        .where(F.col(\"price_clean\").isNotNull())\n",
    "        .where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "    )\n",
    "else:\n",
    "    print(f\"[silver] {city_name}: WARNING price parsing failed for entire city; skipping price filter.\")\n",
    "    filtered_stage_a = latest_only\n",
    "\n",
    "non_null_geo_cnt = filtered_stage_a.where(\n",
    "    F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull()\n",
    ").count()\n",
    "print(f\"[silver] {city_name}: listings with valid lat/lon = {non_null_geo_cnt}\")\n",
    "\n",
    "if non_null_geo_cnt > 0:\n",
    "    filtered_stage_b = filtered_stage_a.where(F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull())\n",
    "    final_filtered = filtered_stage_b if filtered_stage_b.count() > 0 else filtered_stage_a\n",
    "else:\n",
    "    print(f\"[silver] {city_name}: WARNING no usable lat/lon; skipping geo filter.\")\n",
    "    final_filtered = filtered_stage_a\n",
    "\n",
    "final_to_write = final_filtered.drop(\"price_number_str\", \"lat_tmp\", \"lon_tmp\")\n",
    "final_count = final_to_write.count()\n",
    "print(f\"[silver] {city_name}: FINAL rows to merge into silver = {final_count}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 5: Upsert into SILVER listings (merge on (pk_listing_id, city))\n",
    "# ---------------------------------------------------------\n",
    "# ---------------------------------------------------------\n",
    "# Step 5 (fixed): Upsert into SILVER listings (robust merge)\n",
    "# ---------------------------------------------------------\n",
    "if not spark.catalog.tableExists(SILVER_LISTINGS_TABLE):\n",
    "    (final_to_write\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"city\")\n",
    "        .saveAsTable(SILVER_LISTINGS_TABLE))\n",
    "    print(f\"[silver] Created {SILVER_LISTINGS_TABLE} with first batch ({city_name})\")\n",
    "else:\n",
    "    staging = final_to_write\n",
    "\n",
    "    # If silver target uses 'listing_id' but staging only has 'pk_listing_id', map it\n",
    "    if \"listing_id\" not in staging.columns and \"pk_listing_id\" in staging.columns:\n",
    "        staging = staging.withColumn(\"listing_id\", F.col(\"pk_listing_id\"))\n",
    "\n",
    "    staging.createOrReplaceTempView(\"silver_new_batch\")\n",
    "    s_cols = staging.columns\n",
    "    t_cols = [f.name for f in spark.table(SILVER_LISTINGS_TABLE).schema]\n",
    "\n",
    "    # Join keys (adjust if your target uses different keys)\n",
    "    join_keys = [\"listing_id\", \"city\"]\n",
    "\n",
    "    # Intersection of columns to avoid unresolved column references\n",
    "    common_cols = [c for c in s_cols if c in t_cols]\n",
    "\n",
    "    # Ensure join keys are present in the insert list (they must exist on target)\n",
    "    for k in join_keys:\n",
    "        if k not in common_cols and k in s_cols and k in t_cols:\n",
    "            common_cols.append(k)\n",
    "\n",
    "    # Columns to update (don't include join keys)\n",
    "    update_cols = [c for c in common_cols if c not in join_keys]\n",
    "\n",
    "    # If there are no non-key columns to update, set a benign update using join keys (sets to same values)\n",
    "    if update_cols:\n",
    "        set_updates = \", \".join([f\"t.{c} = s.{c}\" for c in update_cols])\n",
    "    else:\n",
    "        # fallback: update the join keys to themselves from source (no-op but satisfies MERGE)\n",
    "        set_updates = \", \".join([f\"t.{k} = s.{k}\" for k in join_keys if k in t_cols and k in s_cols])\n",
    "        if not set_updates:\n",
    "            # extremely defensive: if even that fails, set a no-op using ingestion_timestamp if possible\n",
    "            if \"ingestion_timestamp\" in common_cols:\n",
    "                set_updates = \"t.ingestion_timestamp = s.ingestion_timestamp\"\n",
    "            else:\n",
    "                raise RuntimeError(\"No columns available to update in MERGE; check schemas.\")\n",
    "\n",
    "    insert_cols_list = [c for c in common_cols if c in t_cols]\n",
    "    if not insert_cols_list:\n",
    "        # ensure at least primary key + city for insert\n",
    "        insert_cols_list = [k for k in join_keys if k in s_cols and k in t_cols]\n",
    "        if not insert_cols_list:\n",
    "            raise RuntimeError(\"No valid insert columns found for MERGE into listings; check schemas.\")\n",
    "\n",
    "    insert_cols = \", \".join(insert_cols_list)\n",
    "    insert_vals = \", \".join([f\"s.{c}\" for c in insert_cols_list])\n",
    "\n",
    "    merge_condition = \" AND \".join([f\"t.{k} = s.{k}\" for k in join_keys if k in t_cols and k in s_cols])\n",
    "    if not merge_condition:\n",
    "        raise RuntimeError(\"Cannot build merge condition for listings: missing join key columns on target/source.\")\n",
    "\n",
    "    print(\"[silver] listings MERGE condition:\", merge_condition)\n",
    "    print(\"[silver] listings UPDATE cols:\", update_cols)\n",
    "    print(\"[silver] listings INSERT cols:\", insert_cols_list)\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {SILVER_LISTINGS_TABLE} t\n",
    "        USING silver_new_batch s\n",
    "        ON {merge_condition}\n",
    "        WHEN MATCHED THEN UPDATE SET {set_updates}\n",
    "        WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "    \"\"\")\n",
    "    print(f\"[silver] Merged {city_name} rows into {SILVER_LISTINGS_TABLE}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 6 (fixed): Build / upsert SILVER hosts (robust merge)\n",
    "# ---------------------------------------------------------\n",
    "if \"host_id\" in final_to_write.columns:\n",
    "    silver_hosts_city = (\n",
    "        final_to_write\n",
    "        .filter(F.col(\"host_id\").isNotNull())\n",
    "        .groupBy(\"city\", \"host_id\")\n",
    "        .agg(\n",
    "            F.min(\"host_since_dt\").alias(\"host_since_dt\"),\n",
    "            F.countDistinct(\"pk_listing_id\").alias(\"listings_count\"),\n",
    "            F.first(\"host_name\", ignorenulls=True).alias(\"host_name\"),\n",
    "            F.first(\"host_is_superhost\", ignorenulls=True).alias(\"host_is_superhost\")\n",
    "        )\n",
    "        .withColumn(\"host_since_year\", F.year(\"host_since_dt\"))\n",
    "    )\n",
    "\n",
    "    if not spark.catalog.tableExists(SILVER_HOSTS_TABLE):\n",
    "        (silver_hosts_city\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"city\")\n",
    "            .saveAsTable(SILVER_HOSTS_TABLE))\n",
    "        print(f\"[silver] Created {SILVER_HOSTS_TABLE} with first batch ({city_name})\")\n",
    "    else:\n",
    "        hosts_staging = silver_hosts_city\n",
    "        hosts_staging.createOrReplaceTempView(\"hosts_new_batch\")\n",
    "\n",
    "        s_hcols = hosts_staging.columns\n",
    "        t_hcols = [f.name for f in spark.table(SILVER_HOSTS_TABLE).schema]\n",
    "\n",
    "        # Join keys for hosts\n",
    "        h_join_keys = [\"host_id\", \"city\"]\n",
    "\n",
    "        # columns common between source and target\n",
    "        common_hcols = [c for c in s_hcols if c in t_hcols]\n",
    "\n",
    "        # ensure join keys are present in insert list\n",
    "        for k in h_join_keys:\n",
    "            if k not in common_hcols and k in s_hcols and k in t_hcols:\n",
    "                common_hcols.append(k)\n",
    "\n",
    "        # update columns (exclude join keys)\n",
    "        update_hcols = [c for c in common_hcols if c not in h_join_keys]\n",
    "\n",
    "        if update_hcols:\n",
    "            set_updates_hosts = \", \".join([f\"t.{c} = s.{c}\" for c in update_hcols])\n",
    "        else:\n",
    "            set_updates_hosts = \", \".join([f\"t.{k} = s.{k}\" for k in h_join_keys if k in t_hcols and k in s_hcols])\n",
    "            if not set_updates_hosts:\n",
    "                if \"listings_count\" in common_hcols:\n",
    "                    set_updates_hosts = \"t.listings_count = s.listings_count\"\n",
    "                else:\n",
    "                    raise RuntimeError(\"No columns available to update in MERGE for hosts; check schemas.\")\n",
    "\n",
    "        insert_cols_hosts_list = [c for c in common_hcols if c in t_hcols]\n",
    "        if not insert_cols_hosts_list:\n",
    "            insert_cols_hosts_list = [k for k in h_join_keys if k in s_hcols and k in t_hcols]\n",
    "            if not insert_cols_hosts_list:\n",
    "                raise RuntimeError(\"No valid insert columns found for MERGE into hosts; check schemas.\")\n",
    "\n",
    "        insert_cols_hosts = \", \".join(insert_cols_hosts_list)\n",
    "        insert_vals_hosts = \", \".join([f\"s.{c}\" for c in insert_cols_hosts_list])\n",
    "\n",
    "        merge_condition_hosts = \" AND \".join([f\"t.{k} = s.{k}\" for k in h_join_keys if k in t_hcols and k in s_hcols])\n",
    "        if not merge_condition_hosts:\n",
    "            raise RuntimeError(\"Cannot build merge condition for hosts: missing join key columns on target/source.\")\n",
    "\n",
    "        print(\"[silver] hosts MERGE condition:\", merge_condition_hosts)\n",
    "        print(\"[silver] hosts UPDATE cols:\", update_hcols)\n",
    "        print(\"[silver] hosts INSERT cols:\", insert_cols_hosts_list)\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {SILVER_HOSTS_TABLE} t\n",
    "            USING hosts_new_batch s\n",
    "            ON {merge_condition_hosts}\n",
    "            WHEN MATCHED THEN UPDATE SET {set_updates_hosts}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_cols_hosts}) VALUES ({insert_vals_hosts})\n",
    "        \"\"\")\n",
    "        print(f\"[silver] Merged {city_name} host rows into {SILVER_HOSTS_TABLE}\")\n",
    "else:\n",
    "    print(f\"[silver] WARNING: 'host_id' column not present; skipping hosts table.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 7: Sanity checks\n",
    "# ---------------------------------------------------------\n",
    "print(\"[silver] Row counts by city (listings):\")\n",
    "display(spark.table(SILVER_LISTINGS_TABLE).groupBy(\"city\").count().orderBy(\"city\"))\n",
    "\n",
    "if spark.catalog.tableExists(SILVER_HOSTS_TABLE):\n",
    "    print(\"[silver] Row counts by city (hosts):\")\n",
    "    display(spark.table(SILVER_HOSTS_TABLE).groupBy(\"city\").count().orderBy(\"city\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80b5a50-0a2d-4031-ad78-09010e07a3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import Window\n",
    "\n",
    "# CATALOG = \"workspace\" \n",
    "# BRONZE_TABLE          = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "# SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "# SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "# spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.airbnb_silver\")\n",
    "\n",
    "# # ---- city param ----\n",
    "# dbutils.widgets.text(\"city\", \"Paris\")\n",
    "# city_name = dbutils.widgets.get(\"city\")\n",
    "# print(f\"[silver] Transform for city={city_name}\")\n",
    "\n",
    "# # ---- load bronze for that city ----\n",
    "# bronze_city = (\n",
    "#     spark.table(BRONZE_TABLE)\n",
    "#          .where(F.col(\"city\") == city_name)\n",
    "# )\n",
    "\n",
    "# print(f\"[silver] bronze rows for {city_name}: {bronze_city.count()}\")\n",
    "\n",
    "# # === INSERTED: clean empty list-like strings ('[]', \"['']\", blanks) ===\n",
    "# def clean_empty_list_strings(df, cols):\n",
    "#     for c in cols:\n",
    "#         if c in df.columns:\n",
    "#             df = df.withColumn(\n",
    "#                 c,\n",
    "#                 F.when(\n",
    "#                     F.col(c).isNull() |\n",
    "#                     (F.trim(F.col(c)) == \"\") |\n",
    "#                     (F.trim(F.col(c)) == \"[]\") |\n",
    "#                     (F.trim(F.col(c)) == \"['']\") |\n",
    "#                     (F.length(F.col(c)) <= 4),\n",
    "#                     F.lit(None)\n",
    "#                 ).otherwise(F.col(c))\n",
    "#             )\n",
    "#     return df\n",
    "\n",
    "\n",
    "# cols_to_clean = [\"host_verifications\", \"amenities\"]\n",
    "# bronze_city = clean_empty_list_strings(bronze_city, cols_to_clean)\n",
    "# print(f\"[silver] Cleaned empty placeholders in: {cols_to_clean}\")\n",
    "\n",
    "# price_candidates = [c for c in [\"price\",\"price_x\",\"price_y\",\"cost\",\"nightly_price\"] if c in bronze_city.columns]\n",
    "# lat_candidates   = [c for c in [\"latitude\",\"lat\",\"Latitude\",\"geo_lat\"]               if c in bronze_city.columns]\n",
    "# lon_candidates   = [c for c in [\"longitude\",\"lon\",\"Longitude\",\"lng\",\"geo_lon\"]       if c in bronze_city.columns]\n",
    "\n",
    "# price_col = price_candidates[0] if price_candidates else None\n",
    "# lat_col   = lat_candidates[0]   if lat_candidates else None\n",
    "# lon_col   = lon_candidates[0]   if lon_candidates else None\n",
    "\n",
    "# print(\"[silver] using columns:\",\n",
    "#       \"price_col =\", price_col,\n",
    "#       \"lat_col =\", lat_col,\n",
    "#       \"lon_col =\", lon_col)\n",
    "\n",
    "# num_regex_bath = r'(\\d+(\\.\\d+)?)' \n",
    "\n",
    "# df_typed = bronze_city\n",
    "\n",
    "# df_typed = df_typed.withColumn(\n",
    "#     \"bathrooms_clean\",\n",
    "#     F.when(\n",
    "#         F.col(\"bathrooms_text\").rlike(\"(?i)^\\\\s*half\"),\n",
    "#         F.lit(0.5)\n",
    "#     )\n",
    "#     .when(\n",
    "#         F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"),\n",
    "#         F.lit(None).cast(\"double\")\n",
    "#     )\n",
    "#     .when(\n",
    "#         F.col(\"bathrooms_text\").rlike(num_regex_bath),\n",
    "#         F.regexp_extract(\"bathrooms_text\", num_regex_bath, 1).cast(\"double\")\n",
    "#     )\n",
    "#     .otherwise(F.lit(None).cast(\"double\"))\n",
    "# )\n",
    "\n",
    "# if price_col is not None:\n",
    "#     # normalize commas to dots\n",
    "#     tmp_price = F.regexp_replace(F.col(price_col), \",\", \".\")\n",
    "#     # remove currency symbols/letters/spaces, keep digits and dots\n",
    "#     tmp_price = F.regexp_replace(tmp_price, r\"[^0-9\\.]\", \"\")\n",
    "#     # grab first numeric token\n",
    "#     df_typed = df_typed.withColumn(\n",
    "#         \"price_number_str\",\n",
    "#         F.regexp_extract(tmp_price, r\"(\\d+(\\.\\d+)?)\", 1)\n",
    "#     )\n",
    "#     # cast final numeric\n",
    "#     df_typed = df_typed.withColumn(\n",
    "#         \"price_clean\",\n",
    "#         F.col(\"price_number_str\").cast(\"double\")\n",
    "#     )\n",
    "# else:\n",
    "#     df_typed = df_typed.withColumn(\n",
    "#         \"price_clean\", F.lit(None).cast(\"double\")\n",
    "#     )\n",
    "\n",
    "# df_typed = (\n",
    "#     df_typed\n",
    "#     .withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "#     .withColumn(\"host_since_dt\",   F.to_date(\"host_since\"))\n",
    "#     .withColumn(\"availability_30_int\",  F.col(\"availability_30\").cast(\"int\"))\n",
    "#     .withColumn(\"availability_365_int\", F.col(\"availability_365\").cast(\"int\"))\n",
    "#     .withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    "# )\n",
    "\n",
    "# w_latest = Window.partitionBy(\"id\").orderBy(\n",
    "#     F.col(\"last_scraped_dt\").desc_nulls_last()\n",
    "# )\n",
    "\n",
    "# latest_only = (\n",
    "#     df_typed\n",
    "#     .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "#     .where(F.col(\"rn\") == 1)\n",
    "#     .drop(\"rn\")\n",
    "# )\n",
    "\n",
    "# count_latest = latest_only.count()\n",
    "# print(f\"[silver] {city_name}: rows after latest snapshot per listing_id = {count_latest}\")\n",
    "\n",
    "# if lat_col is not None:\n",
    "#     latest_only = latest_only.withColumn(\"lat_tmp\", F.col(lat_col).cast(\"double\"))\n",
    "# else:\n",
    "#     latest_only = latest_only.withColumn(\"lat_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "# if lon_col is not None:\n",
    "#     latest_only = latest_only.withColumn(\"lon_tmp\", F.col(lon_col).cast(\"double\"))\n",
    "# else:\n",
    "#     latest_only = latest_only.withColumn(\"lon_tmp\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "# non_null_price_cnt = latest_only.where(F.col(\"price_clean\").isNotNull()).count()\n",
    "# print(f\"[silver] {city_name}: listings with non-null price_clean = {non_null_price_cnt}\")\n",
    "\n",
    "# if non_null_price_cnt > 0:\n",
    "#     filtered_stage_a = (\n",
    "#         latest_only\n",
    "#         .where(F.col(\"price_clean\").isNotNull())\n",
    "#         .where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "#     )\n",
    "# else:\n",
    "#     print(f\"[silver] {city_name}: WARNING price parsing failed for entire city, skipping price filter.\")\n",
    "#     filtered_stage_a = latest_only\n",
    "\n",
    "# count_stage_a = filtered_stage_a.count()\n",
    "# print(f\"[silver] {city_name}: rows after price filter logic = {count_stage_a}\")\n",
    "\n",
    "# non_null_geo_cnt = filtered_stage_a.where(\n",
    "#     F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull()\n",
    "# ).count()\n",
    "# print(f\"[silver] {city_name}: listings with valid lat/lon = {non_null_geo_cnt}\")\n",
    "\n",
    "# if non_null_geo_cnt > 0:\n",
    "#     filtered_stage_b = (\n",
    "#         filtered_stage_a\n",
    "#         .where(F.col(\"lat_tmp\").isNotNull())\n",
    "#         .where(F.col(\"lon_tmp\").isNotNull())\n",
    "#     )\n",
    "#     count_stage_b = filtered_stage_b.count()\n",
    "#     print(f\"[silver] {city_name}: rows after geo filter logic = {count_stage_b}\")\n",
    "#     final_filtered = filtered_stage_b if count_stage_b > 0 else filtered_stage_a\n",
    "# else:\n",
    "#     print(f\"[silver] {city_name}: WARNING no usable lat/lon, skipping geo filter.\")\n",
    "#     final_filtered = filtered_stage_a\n",
    "\n",
    "# final_count = final_filtered.count()\n",
    "# print(f\"[silver] {city_name}: FINAL rows to merge into silver = {final_count}\")\n",
    "\n",
    "# final_to_write = final_filtered.drop(\"lat_tmp\", \"lon_tmp\", \"price_number_str\")\n",
    "\n",
    "# if not spark.catalog.tableExists(SILVER_LISTINGS_TABLE):\n",
    "#     (\n",
    "#         final_to_write\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\")\n",
    "#         .partitionBy(\"city\")\n",
    "#         .saveAsTable(SILVER_LISTINGS_TABLE)\n",
    "#     )\n",
    "#     print(f\"[silver] Created {SILVER_LISTINGS_TABLE} with first batch ({city_name})\")\n",
    "# else:\n",
    "#     final_to_write.createOrReplaceTempView(\"silver_new_batch\")\n",
    "\n",
    "#     cols = final_to_write.columns\n",
    "#     merge_condition = \"t.id = s.id AND t.city = s.city\"\n",
    "\n",
    "#     set_updates = \",\\n\".join([f\"t.{c} = s.{c}\" for c in cols])\n",
    "#     insert_cols = \", \".join(cols)\n",
    "#     insert_vals = \", \".join([f\"s.{c}\" for c in cols])\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#         MERGE INTO {SILVER_LISTINGS_TABLE} t\n",
    "#         USING silver_new_batch s\n",
    "#         ON {merge_condition}\n",
    "#         WHEN MATCHED THEN UPDATE SET\n",
    "#         {set_updates}\n",
    "#         WHEN NOT MATCHED THEN INSERT ({insert_cols})\n",
    "#         VALUES ({insert_vals})\n",
    "#     \"\"\")\n",
    "\n",
    "#     print(f\"[silver] Merged {city_name} rows into {SILVER_LISTINGS_TABLE}\")\n",
    "\n",
    "# # --- build / upsert hosts ---\n",
    "# silver_hosts_city = (\n",
    "#     final_to_write\n",
    "#     .filter(F.col(\"host_id\").isNotNull())\n",
    "#     .groupBy(\"city\", \"host_id\")\n",
    "#     .agg(\n",
    "#         F.min(\"host_since_dt\").alias(\"host_since_dt\"),\n",
    "#         F.countDistinct(\"id\").alias(\"listings_count\"),\n",
    "#         F.first(\"host_name\").alias(\"host_name\"),\n",
    "#         F.first(\"host_is_superhost\").alias(\"host_is_superhost\")\n",
    "#     )\n",
    "#     .withColumn(\"host_since_year\", F.year(\"host_since_dt\"))\n",
    "# )\n",
    "\n",
    "# if not spark.catalog.tableExists(SILVER_HOSTS_TABLE):\n",
    "#     (\n",
    "#         silver_hosts_city\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\")\n",
    "#         .partitionBy(\"city\")\n",
    "#         .saveAsTable(SILVER_HOSTS_TABLE)\n",
    "#     )\n",
    "#     print(f\"[silver] Created {SILVER_HOSTS_TABLE} with first batch ({city_name})\")\n",
    "# else:\n",
    "#     silver_hosts_city.createOrReplaceTempView(\"hosts_new_batch\")\n",
    "\n",
    "#     hcols = silver_hosts_city.columns\n",
    "#     merge_condition_hosts = \"t.host_id = s.host_id AND t.city = s.city\"\n",
    "\n",
    "#     set_updates_hosts = \",\\n\".join([f\"t.{c} = s.{c}\" for c in hcols])\n",
    "#     insert_cols_hosts = \", \".join(hcols)\n",
    "#     insert_vals_hosts = \", \".join([f\"s.{c}\" for c in hcols])\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#         MERGE INTO {SILVER_HOSTS_TABLE} t\n",
    "#         USING hosts_new_batch s\n",
    "#         ON {merge_condition_hosts}\n",
    "#         WHEN MATCHED THEN UPDATE SET\n",
    "#         {set_updates_hosts}\n",
    "#         WHEN NOT MATCHED THEN INSERT ({insert_cols_hosts})\n",
    "#         VALUES ({insert_vals_hosts})\n",
    "#     \"\"\")\n",
    "\n",
    "#     print(f\"[silver] Merged {city_name} host rows into {SILVER_HOSTS_TABLE}\")\n",
    "\n",
    "# # --- sanity check final silver ---\n",
    "# display(\n",
    "#     spark.table(SILVER_LISTINGS_TABLE)\n",
    "#          .groupBy(\"city\")\n",
    "#          .count()\n",
    "#          .orderBy(\"city\")\n",
    "# )\n",
    "\n",
    "# display(\n",
    "#     spark.table(SILVER_HOSTS_TABLE)\n",
    "#          .groupBy(\"city\")\n",
    "#          .count()\n",
    "#          .orderBy(\"city\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70af5cd7-dca4-45b5-b53e-c81e06ba568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.table(\"workspace.airbnb_silver.listings\")\n",
    "\n",
    "display(\n",
    "    silver.groupBy(\"city\").agg(\n",
    "        F.sum(F.col(\"host_verifications\").isNull().cast(\"int\")).alias(\"empty_host_verifications\"),\n",
    "        F.sum(F.col(\"amenities\").isNull().cast(\"int\")).alias(\"empty_amenities\")\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {
    "city": {
     "currentValue": "Paris",
     "nuid": "a656cfdc-ed64-49c9-988e-ac2e3c0be910",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
