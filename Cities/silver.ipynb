{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4eec8a3-8a05-4e59-aa19-5525fae92299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.airbnb_gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4872c5da-365a-4d9c-84c3-3b24e0968f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(\"workspace.airbnb_bronze.listings_raw\")\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e435226d-3e15-411a-8db6-c9abaa19579e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "CATALOG = \"workspace\"\n",
    "BRONZE_TABLE          = f\"{CATALOG}.airbnb_bronze.listings_raw\"\n",
    "SILVER_LISTINGS_TABLE = f\"{CATALOG}.airbnb_silver.listings\"\n",
    "SILVER_HOSTS_TABLE    = f\"{CATALOG}.airbnb_silver.hosts\"\n",
    "\n",
    "# City param\n",
    "try:\n",
    "    dbutils.widgets.get(\"city\")\n",
    "except Exception:\n",
    "    dbutils.widgets.text(\"city\", \"Paris\")\n",
    "city_name = dbutils.widgets.get(\"city\")\n",
    "print(f\"[silver] Build for city = {city_name}\")\n",
    "\n",
    "# Load Bronze for the city\n",
    "bronze_city = spark.table(BRONZE_TABLE).where(F.col(\"city\") == city_name)\n",
    "if bronze_city.limit(1).count() == 0:\n",
    "    raise ValueError(f\"No Bronze rows for city='{city_name}'.\")\n",
    "\n",
    "# Cleaning helper: case when we have an empty list in the string, which should be converted to NULL\n",
    "def clean_empty_list_strings(df_in, cols):\n",
    "    df_out = df_in\n",
    "    for c in cols:\n",
    "        if c in df_out.columns:\n",
    "            df_out = df_out.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.col(c).isNull() |\n",
    "                    (F.trim(F.col(c)) == \"\") |\n",
    "                    (F.trim(F.col(c)) == \"[]\") |\n",
    "                    (F.trim(F.col(c)) == \"['']\") |\n",
    "                    (F.length(F.col(c)) <= 4),\n",
    "                    F.lit(None)\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "    return df_out\n",
    "\n",
    "df = clean_empty_list_strings(bronze_city, [\"host_verifications\", \"amenities\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"pk_listing_id\", F.col(\"id\").cast(\"string\"))\n",
    "if \"ingestion_timestamp\" not in df.columns:\n",
    "    df = df.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "\n",
    "num_regex = r'(\\d+(\\.\\d+)?)'\n",
    "\n",
    "# Getting bathrooms count per listing\n",
    "if \"bathrooms_text\" in df.columns:\n",
    "    df = df.withColumn(\n",
    "        \"bathrooms_clean\",\n",
    "        F.when(\n",
    "            F.col(\"bathrooms_text\").rlike(\"(?i)(half\\\\s*bath|shared\\\\s*half)\"),\n",
    "            F.lit(0.5)\n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"bathrooms_text\").isNull() | (F.trim(F.col(\"bathrooms_text\")) == \"\"),\n",
    "            F.lit(None).cast(\"double\")\n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"bathrooms_text\").rlike(num_regex),\n",
    "            F.regexp_extract(\"bathrooms_text\", num_regex, 1).cast(\"double\")\n",
    "        )\n",
    "        .otherwise(F.lit(None).cast(\"double\"))\n",
    "    )\n",
    "else:\n",
    "    df = df.withColumn(\n",
    "        \"bathrooms_clean\",\n",
    "        F.col(\"bathrooms\").cast(\"double\") if \"bathrooms\" in df.columns else F.lit(None).cast(\"double\")\n",
    "    )\n",
    "\n",
    "# price_clean\n",
    "if \"price\" in df.columns:\n",
    "    tmp = F.regexp_replace(F.col(\"price\"), \",\", \".\")\n",
    "    tmp = F.regexp_replace(tmp, r\"[^0-9\\.]\", \"\")\n",
    "    df  = df.withColumn(\"price_number_str\", F.regexp_extract(tmp, num_regex, 1))\n",
    "    df  = df.withColumn(\"price_clean\", F.col(\"price_number_str\").cast(\"double\"))\n",
    "else:\n",
    "    df  = df.withColumn(\"price_clean\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "# dates\n",
    "df = df.withColumn(\"last_scraped_dt\", F.to_date(\"last_scraped\"))\n",
    "df = df.withColumn(\"host_since_dt\",   F.to_date(\"host_since\"))\n",
    "\n",
    "# numerics\n",
    "for c in [\"availability_30\",\"availability_60\",\"availability_90\",\"availability_365\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.withColumn(f\"{c}_int\", F.col(c).cast(\"int\"))\n",
    "\n",
    "if \"review_scores_rating\" in df.columns:\n",
    "    df = df.withColumn(\"review_scores_rating_dbl\", F.col(\"review_scores_rating\").cast(\"double\"))\n",
    "\n",
    "# geo\n",
    "df = df.withColumn(\"lat_tmp\", F.col(\"latitude\").cast(\"double\") if \"latitude\" in df.columns else F.lit(None).cast(\"double\"))\n",
    "df = df.withColumn(\"lon_tmp\", F.col(\"longitude\").cast(\"double\") if \"longitude\" in df.columns else F.lit(None).cast(\"double\"))\n",
    "\n",
    "# Latest snapshot per listing (within city)\n",
    "w = Window.partitionBy(\"pk_listing_id\").orderBy(\n",
    "    F.col(\"last_scraped_dt\").desc_nulls_last(),\n",
    "    F.col(\"ingestion_timestamp\").desc_nulls_last()\n",
    ")\n",
    "latest = df.withColumn(\"rn\", F.row_number().over(w)).where(\"rn = 1\").drop(\"rn\")\n",
    "\n",
    "# Quality filters (optional but deterministic)\n",
    "stage = latest\n",
    "if stage.where(F.col(\"price_clean\").isNotNull()).limit(1).count() > 0:\n",
    "    stage = stage.where((F.col(\"price_clean\") > 0) & (F.col(\"price_clean\") < 5000))\n",
    "if stage.where(F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull()).limit(1).count() > 0:\n",
    "    stage = stage.where(F.col(\"lat_tmp\").isNotNull() & F.col(\"lon_tmp\").isNotNull())\n",
    "\n",
    "final_listings = stage.drop(\"price_number_str\", \"lat_tmp\", \"lon_tmp\")\n",
    "\n",
    "# Hosts aggregate from final_listings\n",
    "hosts_city = None\n",
    "if \"host_id\" in final_listings.columns:\n",
    "    hosts_city = (\n",
    "        final_listings\n",
    "        .filter(F.col(\"host_id\").isNotNull())\n",
    "        .groupBy(\"city\", \"host_id\")\n",
    "        .agg(\n",
    "            F.min(\"host_since_dt\").alias(\"host_since_dt\"),\n",
    "            F.countDistinct(\"pk_listing_id\").alias(\"listings_count\"),\n",
    "            F.first(\"host_name\", ignorenulls=True).alias(\"host_name\"),\n",
    "            F.first(\"host_is_superhost\", ignorenulls=True).alias(\"host_is_superhost\")\n",
    "        )\n",
    "        .withColumn(\"host_since_year\", F.year(\"host_since_dt\"))\n",
    "    )\n",
    "\n",
    "final_listings.createOrReplaceTempView(\"silver_this_city_listings\")\n",
    "if hosts_city is not None:\n",
    "    hosts_city.createOrReplaceTempView(\"silver_this_city_hosts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9092f791-09dd-4222-8b45-669070d36f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(SILVER_LISTINGS_TABLE):\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {SILVER_LISTINGS_TABLE}\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (city)\n",
    "        AS SELECT * FROM silver_this_city_listings WHERE 1=0\n",
    "    \"\"\")\n",
    "else:\n",
    "    details = spark.sql(f\"DESCRIBE DETAIL {SILVER_LISTINGS_TABLE}\").first().asDict()\n",
    "    # Check partitioning\n",
    "    part_cols = [r.col_name for r in spark.sql(f\"DESCRIBE EXTENDED {SILVER_LISTINGS_TABLE}\")\n",
    "                 .where(\"col_name = 'Partitioning'\").collect()]\n",
    "    # Safer check via table properties:\n",
    "    desc = spark.sql(f\"DESCRIBE TABLE {SILVER_LISTINGS_TABLE}\").collect()\n",
    "    is_partitioned = any(\"Partition Columns\" in r[0] and r[1] is not None for r in desc)\n",
    "    if not is_partitioned:\n",
    "        # Rebuild as partitioned by city preserving current data\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {SILVER_LISTINGS_TABLE}\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (city)\n",
    "            AS SELECT * FROM {SILVER_LISTINGS_TABLE}\n",
    "        \"\"\")\n",
    "\n",
    "if hosts_city is not None:\n",
    "    if not spark.catalog.tableExists(SILVER_HOSTS_TABLE):\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {SILVER_HOSTS_TABLE}\n",
    "            USING DELTA\n",
    "            PARTITIONED BY (city)\n",
    "            AS SELECT * FROM silver_this_city_hosts WHERE 1=0\n",
    "        \"\"\")\n",
    "    else:\n",
    "        desc = spark.sql(f\"DESCRIBE TABLE {SILVER_HOSTS_TABLE}\").collect()\n",
    "        is_partitioned = any(\"Partition Columns\" in r[0] and r[1] is not None for r in desc)\n",
    "        if not is_partitioned:\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {SILVER_HOSTS_TABLE}\n",
    "                USING DELTA\n",
    "                PARTITIONED BY (city)\n",
    "                AS SELECT * FROM {SILVER_HOSTS_TABLE}\n",
    "            \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0914a63-13fd-4280-90ab-15372069ac9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def align_to_target(df_src, target_table, partition_col=\"city\"):\n",
    "    t_schema = spark.table(target_table).schema\n",
    "\n",
    "    # Target fields\n",
    "    pfield = next((f for f in t_schema if f.name == partition_col), None)\n",
    "    if pfield is None:\n",
    "        raise ValueError(f\"Partition column '{partition_col}' not found in target table {target_table}.\")\n",
    "\n",
    "    nonpart_fields = [f for f in t_schema if f.name != partition_col]\n",
    "    nonpart_cols   = [f.name for f in nonpart_fields]\n",
    "    target_cols_set = set([f.name for f in t_schema])\n",
    "\n",
    "    df = df_src\n",
    "\n",
    "    # Add any missing target columns (with correct data types)\n",
    "    src_cols = set(df.columns)\n",
    "    for f in nonpart_fields:\n",
    "        if f.name not in src_cols:\n",
    "            df = df.withColumn(f.name, F.lit(None).cast(f.dataType))\n",
    "    if partition_col not in src_cols:\n",
    "        df = df.withColumn(partition_col, F.lit(None).cast(pfield.dataType))\n",
    "\n",
    "    # Drop extra columns that are not in target schema\n",
    "    drop_cols = [c for c in df.columns if c not in target_cols_set]\n",
    "    for c in drop_cols:\n",
    "        df = df.drop(c)\n",
    "\n",
    "    # Cast to target data types\n",
    "    for f in nonpart_fields:\n",
    "        df = df.withColumn(f.name, F.col(f.name).cast(f.dataType))\n",
    "    df = df.withColumn(partition_col, F.col(partition_col).cast(pfield.dataType))\n",
    "\n",
    "    df = df.select(*(nonpart_cols + [partition_col]))\n",
    "    return df, nonpart_cols\n",
    "# listingd\n",
    "aligned_listings_df, listings_nonpart_cols = align_to_target(\n",
    "    final_listings, SILVER_LISTINGS_TABLE, partition_col=\"city\"\n",
    ")\n",
    "aligned_listings_df.createOrReplaceTempView(\"silver_this_city_listings_aligned\")\n",
    "\n",
    "# hosts\n",
    "hosts_nonpart_cols = None\n",
    "if spark.catalog.tableExists(SILVER_HOSTS_TABLE) and \"silver_this_city_hosts\" in [t.name for t in spark.catalog.listTables()]:\n",
    "    aligned_hosts_df, hosts_nonpart_cols = align_to_target(\n",
    "        spark.table(\"silver_this_city_hosts\"), SILVER_HOSTS_TABLE, partition_col=\"city\"\n",
    "    )\n",
    "    aligned_hosts_df.createOrReplaceTempView(\"silver_this_city_hosts_aligned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7653ae24-ac23-43f8-b5b8-4571e64cd84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {SILVER_LISTINGS_TABLE}\n",
    "    PARTITION (city = '{city_name}')\n",
    "    SELECT {', '.join(listings_nonpart_cols)}\n",
    "    FROM silver_this_city_listings_aligned\n",
    "    WHERE city = '{city_name}'\n",
    "\"\"\")\n",
    "\n",
    "if hosts_nonpart_cols is not None:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT OVERWRITE TABLE {SILVER_HOSTS_TABLE}\n",
    "        PARTITION (city = '{city_name}')\n",
    "        SELECT {', '.join(hosts_nonpart_cols)}\n",
    "        FROM silver_this_city_hosts_aligned\n",
    "        WHERE city = '{city_name}'\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ebbb80-14f0-4710-8102-112212f0898c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Row count by city per listing:\")\n",
    "city_counts_df = (\n",
    "    spark.table(SILVER_LISTINGS_TABLE)\n",
    "         .groupBy(\"city\")\n",
    "         .count()\n",
    "         .orderBy(\"city\")\n",
    ")\n",
    "display(city_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70af5cd7-dca4-45b5-b53e-c81e06ba568c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.table(\"workspace.airbnb_silver.listings\")\n",
    "\n",
    "display(\n",
    "    silver.groupBy(\"city\").agg(\n",
    "        F.sum(F.col(\"host_verifications\").isNull().cast(\"int\")).alias(\"empty_host_verifications\"),\n",
    "        F.sum(F.col(\"amenities\").isNull().cast(\"int\")).alias(\"empty_amenities\")\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {
    "city": {
     "currentValue": "Paris",
     "nuid": "a656cfdc-ed64-49c9-988e-ac2e3c0be910",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Paris",
      "label": null,
      "name": "city",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
