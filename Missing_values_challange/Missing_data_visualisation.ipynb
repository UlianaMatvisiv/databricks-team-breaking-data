{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f1d869-8258-4b7e-b419-db942f861af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.table('airbnb.raw.listings')\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71a9d91-c273-4437-90c7-71f0f81693b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25572c59-705d-4af9-9303-7ff378f7beaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc504e3-4772-481c-b770-30e19900bff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_missing_expr(col_name, dtype, include_empty_str):\n",
    "    c = F.col(col_name)\n",
    "    base_null = c.isNull()\n",
    "    if isinstance(dtype, (T.FloatType, T.DoubleType)):\n",
    "        cond = base_null | F.isnan(c)\n",
    "    elif isinstance(dtype, T.StringType):\n",
    "        cond = base_null | (F.length(F.trim(c)) == 0 if include_empty_str else F.lit(False))\n",
    "    else:\n",
    "        cond = base_null\n",
    "    return F.when(cond, 1).otherwise(0)\n",
    "\n",
    "def missing_summary(df, columns=None, include_empty_str=True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns were given\")\n",
    "\n",
    "    total_rows = df.count()\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "\n",
    "    exprs = [F.sum(is_missing_expr(c, schema_map[c], include_empty_str)).alias(c) for c in columns]\n",
    "    null_counts_row = df.select(*exprs).collect()[0].asDict()\n",
    "\n",
    "    rows = []\n",
    "    for c in columns:\n",
    "        nulls = int(null_counts_row[c])\n",
    "        present = total_rows - nulls\n",
    "        null_pct = (nulls / total_rows * 100.0) if total_rows else 0.0\n",
    "        rows.append({\"column\": c, \"present_count\": present, \"null_count\": nulls, \"null_pct\": round(null_pct, 4)})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"null_pct\", ascending=False, ignore_index=True)\n",
    "\n",
    "def missing_matrix(df, columns, max_rows = 5000, include_empty_str = True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns given\")\n",
    "\n",
    "    with_id = df.select(F.monotonically_increasing_id().alias(\"_rid\"), *columns).orderBy(\"_rid\").limit(max_rows)\n",
    "\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "    miss_cols = [is_missing_expr(c, schema_map[c], include_empty_str).alias(c) for c in columns]\n",
    "\n",
    "    bin_df = with_id.select(\"_rid\", *miss_cols).orderBy(\"_rid\").drop(\"_rid\")\n",
    "    pdf = bin_df.toPandas()\n",
    "    return pdf\n",
    "\n",
    "def visualize_missing(df, columns, max_rows = 2000, include_empty_str = True):\n",
    "    \"\"\"\n",
    "    Visualisation that gives basic understanding of amount of pressent and missing data (with % of missing data), plots heatmap showing where missing data is (displays only columns where there are missing values) and bar chart of null % in the data \n",
    "    \"\"\"\n",
    "    summary_pdf = missing_summary(df, columns=columns, include_empty_str=include_empty_str)\n",
    "    # Keeping columns that have some missing values\n",
    "    cols_with_nulls = summary_pdf.loc[summary_pdf[\"null_count\"] > 0, \"column\"].tolist()\n",
    "\n",
    "    if not cols_with_nulls:\n",
    "        print(\"No missing values detected\")\n",
    "        display(spark.createDataFrame(summary_pdf))\n",
    "        return summary_pdf\n",
    "\n",
    "    # Ploting heatmap: grenn - data is present, red - missing data (null value)\n",
    "    mat_pdf = missing_matrix(df, columns=cols_with_nulls, max_rows=max_rows, include_empty_str=include_empty_str)\n",
    "    if not mat_pdf.empty:\n",
    "        mat = mat_pdf.values.astype(np.int8)\n",
    "        fig1, ax1 = plt.subplots(figsize=(12, 10))\n",
    "        cmap = ListedColormap([\"#3ec245\", \"#f21818\"])\n",
    "        norm = BoundaryNorm([-0.5, 0.5, 1.5], cmap.N)\n",
    "        ax1.imshow(mat, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "        ax1.set_title(f\"Missingness heatmap\")\n",
    "        ax1.set_xlabel(\"Columns\")\n",
    "        ax1.set_ylabel(\"Row number\")\n",
    "        ax1.set_xticks(range(len(mat_pdf.columns)))\n",
    "        ax1.set_xticklabels(mat_pdf.columns, rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Ploting bar charts with null % per column\n",
    "    filtered_summary = summary_pdf[summary_pdf[\"column\"].isin(cols_with_nulls)]\n",
    "    if not filtered_summary.empty:\n",
    "        fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
    "        ax2.bar(filtered_summary[\"column\"], filtered_summary[\"null_pct\"])\n",
    "        ax2.set_title(\"Null % per column\")\n",
    "        ax2.set_xlabel(\"Column\")\n",
    "        ax2.set_ylabel(\"Null percentage (%)\")\n",
    "        ax2.set_xticklabels(filtered_summary[\"column\"], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    display(spark.createDataFrame(summary_pdf))\n",
    "    return summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ead71a-c220-41af-b4c3-c4728611702b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = visualize_missing(raw_df, columns=None, max_rows=15000, include_empty_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3724bb-b890-490b-90af-203dcf9b2023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ffab88-d5b8-4fde-a327-ec39ee7885eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def missing_rate_expr(col: str):\n",
    "    return (F.count(F.when(F.col(col).isNull(), 1)) / F.count(F.lit(1))).alias(col)\n",
    "\n",
    "def missing_indicator(col: str):\n",
    "    return F.when(F.col(col).isNull(), 1).otherwise(0).alias(col + \"_miss\")\n",
    "\n",
    "def to_pandas(df, limit=None):\n",
    "    return (df.limit(limit) if isinstance(limit, int) else df).toPandas()\n",
    "\n",
    "SAMPLE_FRAC = 0.20  \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb530c2-b2f8-47ac-bd66-cff96a50bb6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = raw_df.schema\n",
    "\n",
    "type_groups = {\n",
    "    \"numeric\": [f.name for f in schema if isinstance(f.dataType, (T.IntegerType, T.LongType, T.FloatType, T.DoubleType, T.ShortType, T.DecimalType))],\n",
    "    \"string\":  [f.name for f in schema if isinstance(f.dataType, T.StringType)],\n",
    "    \"boolean\": [f.name for f in schema if isinstance(f.dataType, T.BooleanType)],\n",
    "    \"date_ts\": [f.name for f in schema if isinstance(f.dataType, (T.DateType, T.TimestampType))]\n",
    "}\n",
    "\n",
    "stats_rows: List[Tuple[str, float, int]] = []\n",
    "for tname, cols in type_groups.items():\n",
    "    if not cols:\n",
    "        continue\n",
    "    exprs = [missing_rate_expr(c) for c in cols]\n",
    "    row = raw_df.agg(*exprs).collect()[0].asDict()\n",
    "    mean_missing = sum(row.values()) / len(cols)\n",
    "    stats_rows.append((tname, float(mean_missing), len(cols)))\n",
    "\n",
    "missing_by_type_df = spark.createDataFrame(stats_rows, [\"type\", \"avg_missing_rate\", \"n_cols\"])\n",
    "display(missing_by_type_df.orderBy(F.desc(\"avg_missing_rate\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a483f7-5b48-4265-b93b-462f916c179a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = to_pandas(missing_by_type_df.orderBy(F.desc(\"avg_missing_rate\")))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(pdf[\"type\"], pdf[\"avg_missing_rate\"])\n",
    "plt.title(\"Average Missing Rate by Feature Type\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Avg Missing Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68c669a-5321-4758-9f19-a15859b517b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = raw_df.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in raw_df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "cols_with_nulls = [c for c, n in null_counts.items() if n and n > 0]\n",
    "\n",
    "miss_df = raw_df.select([missing_indicator(c) for c in cols_with_nulls])\n",
    "miss_df_sample = miss_df.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n",
    "\n",
    "pdf = miss_df_sample.toPandas()\n",
    "\n",
    "corr = pdf.corr(numeric_only=True)\n",
    "corr.index.name = \"col\"\n",
    "corr.columns.name = \"col\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af39574e-c436-4371-b144-81c8bf8fa58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 30\n",
    "miss_rates = miss_df_sample.agg(*[F.mean(c).alias(c) for c in miss_df_sample.columns]).collect()[0].asDict()\n",
    "top_cols = [k for k,_ in sorted(miss_rates.items(), key=lambda kv: kv[1], reverse=True)[:N]]\n",
    "\n",
    "corr_top = corr.loc[top_cols, top_cols].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "im = plt.imshow(corr_top, aspect='auto')\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(top_cols)), labels=top_cols, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(top_cols)), labels=top_cols)\n",
    "plt.title(\"Co-missingness Correlation (Top N columns)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc66974f-5161-4b24-b171-b04886868a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_col = \"area\" if \"area\" in raw_df.columns else \"neighbourhood_cleansed\"\n",
    "\n",
    "geo_missing = (\n",
    "    raw_df.groupBy(geo_col)\n",
    "    .agg(\n",
    "        (F.count(F.when(F.col(\"license\").isNull(), 1)) / F.count(F.lit(1))).alias(\"license_missing_rate\"),\n",
    "        (F.count(F.when(F.col(\"review_scores_rating\").isNull(), 1)) / F.count(F.lit(1))).alias(\"review_missing_rate\"),\n",
    "        F.count(F.lit(1)).alias(\"n\")\n",
    "    )\n",
    "    .filter(F.col(geo_col).isNotNull())\n",
    ")\n",
    "\n",
    "top_geo = geo_missing.orderBy(F.desc(\"n\")).limit(20)\n",
    "display(top_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec839451-ee73-413f-8451-fde9f539e5e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_geo = to_pandas(top_geo.orderBy(F.desc(\"n\")))\n",
    "x = range(len(pdf_geo))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(x, pdf_geo[\"license_missing_rate\"], width=0.4)\n",
    "plt.xticks(x, pdf_geo[geo_col], rotation=90)\n",
    "plt.title(\"License Missing Rate by Area (Top 20 by volume)\")\n",
    "plt.ylabel(\"Missing Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(x, pdf_geo[\"review_missing_rate\"], width=0.4)\n",
    "plt.xticks(x, pdf_geo[geo_col], rotation=90)\n",
    "plt.title(\"Review Score Missing Rate by Area (Top 20 by volume)\")\n",
    "plt.ylabel(\"Missing Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Missing_data_visualisation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
