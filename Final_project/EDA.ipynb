{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4b388d-3818-4331-97ea-b341edc91566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from typing import List, Tuple\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a49d056-8f43-4aa9-b52f-6c35c2373687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/steam_project/steam_raw_data/raw_data/games.csv\"\n",
    "\n",
    "df_steam = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    .csv(path)\n",
    ")\n",
    "display(df_steam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9263c9-ba7c-4c66-b6a4-887bbe9e54ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unfortunately, we see the shift in the original data: Discount DLC count contains unreliable data, while its true values are stored in the neighbouring column on the right. The pattern continues even further till the end.\n",
    "\n",
    "It can be explained by the flaw in original transformation to csv, as json analog of the file does not contain the data current Discount DLC count column has. \n",
    "\n",
    "With this, we need to transform our data for further work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ae5a47-7de2-4eec-a3fc-de25961376e5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Header image\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Website\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Support url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Screenshots\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1765183205309}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols = df_steam.columns\n",
    "i = cols.index(\"DiscountDLC count\")\n",
    "\n",
    "new_cols = []\n",
    "\n",
    "# Columns before the shift must stay the same (ID, name, relese date, etc)\n",
    "for c in cols[:i]:\n",
    "    new_cols.append(F.col(c).alias(c))\n",
    "\n",
    "# Shift: now column[i] = values from column[i+1]\n",
    "for j in range(i, len(cols)-1):\n",
    "    new_cols.append(F.col(cols[j+1]).alias(cols[j]))\n",
    "\n",
    "# Last column gets NULL because nothing shifts into it\n",
    "new_cols.append(F.lit(None).alias(cols[-1]))\n",
    "\n",
    "df_steam_fixed = df_steam.select(*new_cols)\n",
    "drop_col = df_steam_fixed.columns[-1]\n",
    "df_steam_fixed = df_steam_fixed.drop(drop_col)\n",
    "display(df_steam_fixed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c922b7-03f8-488e-a084-ae88ba86ce96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's make aditional changes: transform numerical values from string, change tags, categories and genres to arrays for ferther use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6c516d-e6ff-41ac-bd15-1f777e774373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Transforming columns to float\n",
    "cols_to_float = [\"Peak CCU\", \"Required age\", \"DiscountDLC count\", \"Metacritic score\", \"User score\", \"Positive\", \"Negative\", \"Score rank\", \"Achievements\", \"Recommendations\", \"Average playtime forever\", \"Average playtime two weeks\", \"Median playtime forever\", \"Median playtime two weeks\"]\n",
    "\n",
    "for c in cols_to_float:\n",
    "    df_steam_fixed = df_steam_fixed.withColumn(c, F.round(F.col(c).cast(\"float\"),2))\n",
    "\n",
    "# Separate case for Price, because we want it rounded as it was in original\n",
    "df_steam_fixed = df_steam_fixed.withColumn(\n",
    "    \"Price\", F.col(\"Price\").cast(DecimalType(10, 2))\n",
    ")\n",
    "\n",
    "# transforming string format to arrays\n",
    "cols_to_array = [\"Categories\", \"Genres\", \"Tags\"]\n",
    "for c in cols_to_array:\n",
    "    df_steam_fixed = df_steam_fixed.withColumn(c, F.split(F.col(c), \",\"))\n",
    "\n",
    "\n",
    "# Separate case for languages that have pseudo arrays \n",
    "cols_to_ar = [\"Supported languages\", \"Full audio languages\"]\n",
    "\n",
    "for c in cols_to_ar:\n",
    "    df_steam_fixed = (\n",
    "        df_steam_fixed\n",
    "        .withColumn(c, F.regexp_replace(F.col(c), \"'\", '\"'))\n",
    "        .withColumn(c, F.from_json(F.col(c), ArrayType(StringType())))\n",
    "    )\n",
    "\n",
    "display(df_steam_fixed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fb9e4c-8bc8-418e-9c9a-f9b451f38797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Still, we need to look into Release dates and their format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe8d6d3-f59b-4bab-9ac1-501bbba9d408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_steam_fixed.select(\n",
    "    F.min(\"Release date\").alias(\"earliest_release\"),\n",
    "    F.max(\"Release date\").alias(\"latest_release\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d147a750-88e7-491b-b281-03e09c02d520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see that currently the latest release date is listed as Sep 9 ,2024, wgich is false, as we know that data has released games from 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61aa798-0fc6-482a-a2be-5da63240a8e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_df = df_steam_fixed.filter(F.col(\"AppID\") == \"3183790\")\n",
    "display(row_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a438f5-7c0c-4360-a083-7fc2b49f519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This means we also need to reformat the release data.\n",
    "\n",
    "We know from visually exploring the data, that there are at least 2 formats of dates: MMM d, yyyy and MMM YYYY. In cases where we can find MMM YYYY format, we decided to add date 15th of the month as the day and unify format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fe92c6-855f-4472-9a49-fc522697954f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "col = F.col(\"Release date\")\n",
    "\n",
    "df_steam_fixed = df_steam_fixed.withColumn(\n",
    "    \"Release date\",\n",
    "    F.when(\n",
    "        # Case 1: full date like Apr 10, 2023\n",
    "        col.rlike(r\"^[A-Za-z]{3} [0-9]{1,2}, [0-9]{4}$\"),\n",
    "        F.to_date(col, \"MMM d, yyyy\")\n",
    "    ).when(\n",
    "        # Case 2: month + year like May 2020\n",
    "        col.rlike(r\"^[A-Za-z]{3} [0-9]{4}$\"),\n",
    "        F.date_add(F.to_date(col, \"MMM yyyy\"), 14)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501627c4-b9c8-4516-88a2-ca905a574d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's chek the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdca6e0-3b93-401d-9020-1026e49f3248",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Header image\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Website\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Support url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Screenshots\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1765195041852}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_steam_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479e4cc9-ebb0-4528-b67b-6ba3aba86852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_steam_fixed.select(\n",
    "    F.min(\"Release date\").alias(\"earliest_release\"),\n",
    "    F.max(\"Release date\").alias(\"latest_release\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fce7e2b-f757-423d-8107-2e2a6d36759f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we see true result on release dates and can get accurate statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec34aa20-57d6-404e-af61-08e3fd369c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Missing values exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c6112b-2cea-4f10-9773-2978ce5bb89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now as we have more or less reliable data, let's look into the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8f795e-2663-43ce-9c32-56010e7a2f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_missing_expr(col_name, dtype, include_empty_str):\n",
    "    c = F.col(col_name)\n",
    "    base_null = c.isNull()\n",
    "    if isinstance(dtype, (T.FloatType, T.DoubleType)):\n",
    "        cond = base_null | F.isnan(c)\n",
    "    elif isinstance(dtype, T.StringType):\n",
    "        cond = base_null | (F.length(F.trim(c)) == 0 if include_empty_str else F.lit(False))\n",
    "    else:\n",
    "        cond = base_null\n",
    "    return F.when(cond, 1).otherwise(0)\n",
    "\n",
    "def missing_summary(df, columns=None, include_empty_str=True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns were given\")\n",
    "\n",
    "    total_rows = df.count()\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "\n",
    "    exprs = [F.sum(is_missing_expr(c, schema_map[c], include_empty_str)).alias(c) for c in columns]\n",
    "    null_counts_row = df.select(*exprs).collect()[0].asDict()\n",
    "\n",
    "    rows = []\n",
    "    for c in columns:\n",
    "        nulls = int(null_counts_row[c])\n",
    "        present = total_rows - nulls\n",
    "        null_pct = (nulls / total_rows * 100.0) if total_rows else 0.0\n",
    "        rows.append({\"column\": c, \"present_count\": present, \"null_count\": nulls, \"null_pct\": round(null_pct, 4)})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"null_pct\", ascending=False, ignore_index=True)\n",
    "\n",
    "def missing_matrix(df, columns, max_rows = 5000, include_empty_str = True):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    columns = [c for c in columns if c in df.columns]\n",
    "    if not columns:\n",
    "        raise ValueError(\"No valid columns given\")\n",
    "\n",
    "    with_id = df.select(F.monotonically_increasing_id().alias(\"_rid\"), *columns).orderBy(\"_rid\").limit(max_rows)\n",
    "\n",
    "    schema_map = {f.name: f.dataType for f in df.schema.fields}\n",
    "    miss_cols = [is_missing_expr(c, schema_map[c], include_empty_str).alias(c) for c in columns]\n",
    "\n",
    "    bin_df = with_id.select(\"_rid\", *miss_cols).orderBy(\"_rid\").drop(\"_rid\")\n",
    "    pdf = bin_df.toPandas()\n",
    "    return pdf\n",
    "\n",
    "def visualize_missing(df, columns, max_rows = 2000, include_empty_str = True):\n",
    "    \"\"\"\n",
    "    Visualisation that gives basic understanding of amount of pressent and missing data (with % of missing data), plots heatmap showing where missing data is (displays only columns where there are missing values) and bar chart of null % in the data \n",
    "    \"\"\"\n",
    "    summary_pdf = missing_summary(df, columns=columns, include_empty_str=include_empty_str)\n",
    "    # Keeping columns that have some missing values\n",
    "    cols_with_nulls = summary_pdf.loc[summary_pdf[\"null_count\"] > 0, \"column\"].tolist()\n",
    "\n",
    "    if not cols_with_nulls:\n",
    "        print(\"No missing values detected\")\n",
    "        display(spark.createDataFrame(summary_pdf))\n",
    "        return summary_pdf\n",
    "\n",
    "    # Ploting heatmap: green - data is present, red - missing data (null value)\n",
    "    mat_pdf = missing_matrix(df, columns=cols_with_nulls, max_rows=max_rows, include_empty_str=include_empty_str)\n",
    "    if not mat_pdf.empty:\n",
    "        mat = mat_pdf.values.astype(np.int8)\n",
    "        fig1, ax1 = plt.subplots(figsize=(12, 10))\n",
    "        cmap = ListedColormap([\"#1F449C\", \"#F05039\"])\n",
    "        norm = BoundaryNorm([-0.5, 0.5, 1.5], cmap.N)\n",
    "        ax1.imshow(mat, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "        ax1.set_title(f\"Missingness heatmap\")\n",
    "        ax1.set_xlabel(\"Columns\")\n",
    "        ax1.set_ylabel(\"Row number\")\n",
    "        ax1.set_xticks(range(len(mat_pdf.columns)))\n",
    "        ax1.set_xticklabels(mat_pdf.columns, rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Ploting bar charts with null % per column\n",
    "    filtered_summary = summary_pdf[summary_pdf[\"column\"].isin(cols_with_nulls)]\n",
    "    if not filtered_summary.empty:\n",
    "        fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
    "        ax2.bar(filtered_summary[\"column\"], filtered_summary[\"null_pct\"])\n",
    "        ax2.set_title(\"Null % per column\")\n",
    "        ax2.set_xlabel(\"Column\")\n",
    "        ax2.set_ylabel(\"Null percentage (%)\")\n",
    "        ax2.set_xticklabels(filtered_summary[\"column\"], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    display(spark.createDataFrame(summary_pdf))\n",
    "    return summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe5f7a2-ff76-497f-bff4-4e92bd8adef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary = visualize_missing(df_steam_fixed, columns=None, max_rows=1500000, include_empty_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701f4a2c-7c67-4686-b0fd-e0f724bc8920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def missing_rate_expr(col: str):\n",
    "    return (F.count(F.when(F.col(col).isNull(), 1)) / F.count(F.lit(1))).alias(col)\n",
    "\n",
    "def missing_indicator(col: str):\n",
    "    return F.when(F.col(col).isNull(), 1).otherwise(0).alias(col + \"_miss\")\n",
    "\n",
    "def to_pandas(df, limit=None):\n",
    "    return (df.limit(limit) if isinstance(limit, int) else df).toPandas()\n",
    "\n",
    "SAMPLE_FRAC = 0.20  \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e937fe4-df0d-446b-b757-d77ef5b19d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_counts = df_steam_fixed.select([\n",
    "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_steam_fixed.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "cols_with_nulls = [c for c, n in null_counts.items() if n and n > 0]\n",
    "\n",
    "miss_df = df_steam_fixed.select([missing_indicator(c) for c in cols_with_nulls])\n",
    "miss_df_sample = miss_df.sample(withReplacement=False, fraction=SAMPLE_FRAC, seed=SEED)\n",
    "\n",
    "pdf = miss_df_sample.toPandas()\n",
    "\n",
    "corr = pdf.corr(numeric_only=True)\n",
    "corr.index.name = \"col\"\n",
    "corr.columns.name = \"col\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce03e28f-52fe-40bb-ae17-fe1bc392888c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "miss_rates = miss_df_sample.agg(*[F.mean(c).alias(c) for c in miss_df_sample.columns]).collect()[0].asDict()\n",
    "top_cols = [k for k,_ in sorted(miss_rates.items(), key=lambda kv: kv[1], reverse=True)[:N]]\n",
    "\n",
    "corr_top = corr.loc[top_cols, top_cols].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "im = plt.imshow(corr_top, aspect='auto')\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(top_cols)), labels=top_cols, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(top_cols)), labels=top_cols)\n",
    "plt.title(\"Co-missingness Correlation (Top N columns)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c964696f-59e3-4a8a-8f19-603e938d19fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hidden null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0625d56-80ab-4aa7-bd48-5c3232cd051d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is quite common for datasets to have so-called by or team hidden null values: values that cannot be identified as empty by the system, but in relity don't hold any value. \n",
    "\n",
    "As hidden nulls we concider empty lists, Strings like \"none\", \"N/A\", ? etc.\n",
    "\n",
    "Let's look into it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e443715-1fd4-47b7-9591-ac0c7cf248f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols_to_check = [\n",
    "    \"Supported languages\",\n",
    "    \"Full audio languages\",\n",
    "    \"Categories\",\n",
    "    \"Genres\",\n",
    "    \"Tags\",\n",
    "]\n",
    "\n",
    "conditions = {}\n",
    "\n",
    "for c in cols_to_check:\n",
    "    non_empty_filtered = F.expr(f\"filter(`{c}`, x -> trim(x) != '')\")\n",
    "\n",
    "    conditions[c] = (\n",
    "        F.col(c).isNull()\n",
    "        | (F.size(F.col(c)) == 0)\n",
    "        | (F.size(non_empty_filtered) == 0)\n",
    "    )\n",
    "\n",
    "combined_condition = None\n",
    "for cond in conditions.values():\n",
    "    combined_condition = cond if combined_condition is None else (combined_condition | cond)\n",
    "\n",
    "filtered_df = df_steam_fixed.filter(combined_condition).select(*cols_to_check)\n",
    "\n",
    "display(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6221afa7-5811-4a4d-ab54-8d9643328796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, many games don't have audio translation, which is still considered as not empty field. \n",
    "\n",
    "The missingness can be explained by a simple fact that not all games have dialogues, so there is no need for audio adaptation\n",
    "\n",
    "Still, in the furhter transformation we will have to work with this empty lists and change them to **null**\n",
    "\n",
    "Let's move to other cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab2b51e-cfff-48bf-bb09-9d6a6571f04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import or_\n",
    "\n",
    "# Paterns we are looking for\n",
    "null_patterns = [\n",
    "    r\"^$\",                 # empty string\n",
    "    r\"^\\s+$\",              # whitespace\n",
    "    r\"(?i)^null$\",         # null / NULL / Null\n",
    "    r\"(?i)^none$\",         # none / NONE\n",
    "    r\"(?i)^n/a$\",          # n/a / N/A\n",
    "    r\"(?i)^na$\",           # na / NA\n",
    "    r\"^\\?$\",               # ?\n",
    "]\n",
    "\n",
    "string_cols = [c for c, t in df_steam_fixed.dtypes if t == \"string\"]\n",
    "\n",
    "print(\"String columns being checked:\")\n",
    "print(string_cols)\n",
    "\n",
    "conditions = []\n",
    "\n",
    "for c in string_cols:\n",
    "    for pattern in null_patterns:\n",
    "        conditions.append(F.col(c).rlike(pattern))\n",
    "\n",
    "if not conditions:\n",
    "    print(\"No string columns found to check.\")\n",
    "else:\n",
    "    combined_condition = reduce(or_, conditions)\n",
    "    df_null_like = df_steam_fixed.filter(combined_condition)\n",
    "\n",
    "    print(\"Number of rows with null-like values in at least one column:\", df_null_like.count())\n",
    "\n",
    "    display(df_null_like)\n",
    "\n",
    "    print(\"\\nPer-column counts of null-like values:\")\n",
    "    for c in string_cols:\n",
    "        col_condition = reduce(or_, [F.col(c).rlike(p) for p in null_patterns])\n",
    "        cnt = df_steam_fixed.filter(col_condition).count()\n",
    "        if cnt > 0:\n",
    "            print(f\"{c}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd47e49-5c5b-4bb9-a711-40e96feb0c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, there are some more hidden null values in data that we will have to clean in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b3a7dd-8d22-4bf5-871c-930b075fba4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac537f26-10b2-40c6-ad42-cf15dc9cfea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb700b2-bfab-420e-adeb-cbd780d238ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since we now have more-or-less clear understanding of our data and it's limitations, let's explore information that we can gain from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35135b71-7086-40b9-bba0-c3cecab41d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_genres = df_steam_fixed.select(F.explode(\"Genres\").alias(\"Genre\"))\n",
    "\n",
    "df_genre_counts = (\n",
    "    df_genres.groupBy(\"Genre\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "display(df_genre_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5c8b8f-f0db-4612-a8e0-3a077e1bb42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top5_pd = df_genre_counts.toPandas()   # convert to pandas for matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "genres = top5_pd[\"Genre\"]\n",
    "counts = top5_pd[\"count\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bars = plt.barh(genres, counts)\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(\n",
    "        width + max(counts) * 0.02,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{int(width)}\",\n",
    "        va='center'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Number of games\")\n",
    "plt.ylabel(\"Genres\")\n",
    "plt.title(\"Top 5 Steam Genres (by number of games)\")\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, max(counts) * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec22afe-1f35-44a2-a7eb-5b3a599398e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cat = df_steam_fixed.select(F.explode(\"Categories\").alias(\"Category\"))\n",
    "\n",
    "df_cat_counts = (\n",
    "    df_cat.groupBy(\"Category\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "display(df_cat_counts)\n",
    "top5_cat = df_cat_counts.toPandas() \n",
    "category = top5_cat[\"Category\"]\n",
    "counts = top5_cat[\"count\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bars = plt.barh(category, counts)\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(\n",
    "        width + max(counts) * 0.02,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{int(width)}\",\n",
    "        va='center'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Number of games\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.title(\"Top 5 Steam Categories (by number of games)\")\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, max(counts) * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770f215b-624e-461a-bfc3-6076fb143e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257592ec-759a-4312-bc47-4b3f6a997c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_year_counts = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .groupBy(\"year\")\n",
    "        .count()\n",
    "        .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "df_year_counts_pd = df_year_counts.toPandas()\n",
    "df_year_counts_pd\n",
    "years = df_year_counts_pd[\"year\"]\n",
    "counts = df_year_counts_pd[\"count\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(years, counts)\n",
    "\n",
    "# Add labels above each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,   # center the label\n",
    "        height,                            # place above the bar\n",
    "        str(int(height)),                  # label value\n",
    "        ha='center', \n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of games released\")\n",
    "plt.title(\"Game releases per year\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "134cd502-008f-43a2-b65a-7bf9163ab357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Year + exploded genres\n",
    "df_genre_year = (\n",
    "    df_steam_fixed\n",
    "    .withColumn(\"year\", F.year(\"Release date\"))\n",
    "    .select(\"year\", F.explode(\"Genres\").alias(\"genre\"))\n",
    "    .filter(F.col(\"year\").isNotNull() & F.col(\"genre\").isNotNull())\n",
    ")\n",
    "\n",
    "# 2. Count games per (year, genre)\n",
    "df_genre_counts = (\n",
    "    df_genre_year\n",
    "    .groupBy(\"year\", \"genre\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# 3. For each year, keep only the genre with max count\n",
    "w = Window.partitionBy(\"year\").orderBy(F.col(\"count\").desc())\n",
    "\n",
    "df_top_genre_per_year = (\n",
    "    df_genre_counts\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "top_genre_pd = df_top_genre_per_year.toPandas()\n",
    "years = top_genre_pd[\"year\"]\n",
    "counts = top_genre_pd[\"count\"]\n",
    "genres = top_genre_pd[\"genre\"]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "bars = plt.bar(years, counts)\n",
    "\n",
    "# Add genre labels above each bar\n",
    "for bar, genre in zip(bars, genres):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height,\n",
    "        genre,\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10,\n",
    "        rotation=45\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Games (for top genre)\")\n",
    "plt.title(\"Most Popular Game Genre per Year (by Number of Releases)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, max(counts) + 1500)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14b02bc-f2a6-4cd0-826a-010c0a7a256c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "target_genres = [\"Action\", \"Indie\", \"Casual\"]\n",
    "\n",
    "# Explode genres + extract year\n",
    "df_genre_year = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .select(\"year\", F.explode(\"Genres\").alias(\"genre\"))\n",
    "        .filter(\n",
    "            (F.col(\"year\") >= 2010) & (F.col(\"year\") < 2025) &\n",
    "            F.col(\"genre\").isin(target_genres)\n",
    "        )\n",
    ")\n",
    "\n",
    "# Count games per (year, genre)\n",
    "df_counts = (\n",
    "    df_genre_year\n",
    "        .groupBy(\"year\", \"genre\")\n",
    "        .count()\n",
    "        .orderBy(\"year\", \"genre\")\n",
    ")\n",
    "\n",
    "df_counts_pd = df_counts.toPandas()\n",
    "df_pivot = df_counts_pd.pivot(index=\"year\", columns=\"genre\", values=\"count\").fillna(0)\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(df_pivot.index, df_pivot[\"Action\"], marker=\"o\", label=\"Action\")\n",
    "plt.plot(df_pivot.index, df_pivot[\"Indie\"], marker=\"o\", label=\"Indie\")\n",
    "plt.plot(df_pivot.index, df_pivot[\"Casual\"], marker=\"o\", label=\"Casual\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.title(\"Number of Action, Indie, and Casual games released (2010-2024)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34bf563-9df1-48bb-b906-29024f7e0ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_quarter = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"quarter\", F.quarter(\"Release date\"))\n",
    "        .groupBy(\"quarter\")\n",
    "        .count()\n",
    "        .orderBy(\"quarter\")\n",
    "        .toPandas()\n",
    ")\n",
    "\n",
    "quarters = df_quarter[\"quarter\"].astype(str)\n",
    "counts = df_quarter[\"count\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(quarters, counts)\n",
    "\n",
    "# Add labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        str(int(height)),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Quarter\")\n",
    "plt.ylabel(\"Number of Games Released\")\n",
    "plt.title(\"Distribution of Game Releases by Quarter\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeced704-9b20-4ae2-97d9-73cfaa93ffdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_games = df_steam_fixed.count()\n",
    "\n",
    "df_platform_support = (\n",
    "    df_steam_fixed.agg(\n",
    "        (F.sum(F.when(F.col(\"Windows\") == \"True\", 1).otherwise(0)) / total_games * 100).alias(\"Windows\"),\n",
    "        (F.sum(F.when(F.col(\"Mac\") == \"True\", 1).otherwise(0)) / total_games * 100).alias(\"Mac\"),\n",
    "        (F.sum(F.when(F.col(\"Linux\") == \"True\", 1).otherwise(0)) / total_games * 100).alias(\"Linux\")\n",
    "    )\n",
    ").toPandas().T\n",
    "\n",
    "df_platform_support.columns = [\"percentage\"]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "platforms = df_platform_support.index\n",
    "percentages = df_platform_support[\"percentage\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(platforms, percentages, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
    "\n",
    "# Add % labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height + 1,\n",
    "        f\"{height:.1f}%\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.ylabel(\"Games %\")\n",
    "plt.title(\"Platform Support: Windows, Mac, Linux\")\n",
    "plt.ylim(0, max(percentages) + 10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d606095-1180-4326-bda1-bcf83f8fc3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring games by prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbc16f3-905a-47e5-8a97-bccc76cba9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_price_counts = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"is_free\", F.when(F.col(\"Price\") == 0, \"Free\").otherwise(\"Paid\"))\n",
    "        .groupBy(\"is_free\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_price_counts_pd = df_price_counts.toPandas()\n",
    "\n",
    "labels = df_price_counts_pd[\"is_free\"]\n",
    "sizes = df_price_counts_pd[\"count\"]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Free vs Paid games\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84454ca7-695f-489c-b545-6bf91cfdcb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df_year_fp = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .withColumn(\"type\", F.when(F.col(\"Price\") == 0, \"Free\").otherwise(\"Paid\"))\n",
    "        .groupBy(\"year\", \"type\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_year_fp_pd = (\n",
    "    df_year_fp\n",
    "        .groupBy(\"year\")\n",
    "        .pivot(\"type\", [\"Free\", \"Paid\"])\n",
    "        .sum(\"count\")\n",
    "        .fillna(0)\n",
    "        .orderBy(\"year\")\n",
    "        .toPandas()\n",
    ")\n",
    "df_year_fp_pd[\"total\"] = df_year_fp_pd[\"Free\"] + df_year_fp_pd[\"Paid\"]\n",
    "df_year_fp_pd[\"free_pct\"] = df_year_fp_pd[\"Free\"] / df_year_fp_pd[\"total\"] * 100\n",
    "df_year_fp_pd[\"paid_pct\"] = df_year_fp_pd[\"Paid\"] / df_year_fp_pd[\"total\"] * 100\n",
    "years = df_year_fp_pd[\"year\"]\n",
    "free_pct = df_year_fp_pd[\"free_pct\"]\n",
    "paid_pct = df_year_fp_pd[\"paid_pct\"]\n",
    "\n",
    "x = np.arange(len(years)) * 1.3\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "bars_free = plt.bar(x, free_pct, label=\"Free (%)\")\n",
    "bars_paid = plt.bar(x, paid_pct, bottom=free_pct, label=\"Paid (%)\")\n",
    "\n",
    "plt.xticks(x, years, rotation=45)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Percentage of Games\")\n",
    "plt.title(\"Share of Free-to-Play vs Paid Games Over Time\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa50532a-7dab-4a79-bc24-4d29715fac8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Most expensive games/apps in Steam\n",
    "df_most_expencive = (\n",
    "    df_steam_fixed\n",
    "        .select(\"Name\", \"Price\", \"Genres\", \"Estimated owners\", \"Release date\")\n",
    "        .orderBy(F.col(\"Price\").desc())\n",
    "        .limit(6)\n",
    ")\n",
    "display(df_most_expencive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "071065dc-e941-4a37-93d6-07dd8fdd02c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_price_year = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .groupBy(\"year\")\n",
    "        .agg(F.avg(\"Price\").alias(\"avg_price\"))\n",
    "        .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "df_price_year_pd = df_price_year.toPandas()\n",
    "\n",
    "years = df_price_year_pd[\"year\"]\n",
    "avg_prices = df_price_year_pd[\"avg_price\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(years, avg_prices, marker='o')\n",
    "\n",
    "# Add labels on each point\n",
    "for x, y in zip(years, avg_prices):\n",
    "    plt.text(\n",
    "        x, y, \n",
    "        f\"{y:.2f}\", \n",
    "        ha='center', \n",
    "        va='bottom',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average price (USD)\")\n",
    "plt.title(\"Average game prices per years\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6ddff8-6cf0-411f-b4da-d42aee88ba02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "price_series = df_price_year_pd.set_index(\"year\")[\"avg_price\"].astype(float) # from average price per year plot\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(price_series, order=(1, 1, 1))\n",
    "results = model.fit()\n",
    "\n",
    "# Forecast next 5 years\n",
    "n_forecast = 5\n",
    "forecast_res = results.get_forecast(steps=n_forecast)\n",
    "forecast_mean = forecast_res.predicted_mean\n",
    "\n",
    "last_year = int(price_series.index.max())\n",
    "future_years = list(range(last_year + 1, last_year + 1 + n_forecast))\n",
    "\n",
    "forecast_mean.index = future_years\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Historical\n",
    "plt.plot(price_series.index, price_series.values, marker=\"o\", label=\"Historical avg price\")\n",
    "\n",
    "# Forecast\n",
    "plt.plot(forecast_mean.index, forecast_mean.values, marker=\"o\", linestyle=\"--\", label=\"Forecast avg price\")\n",
    "\n",
    "for x, y in zip(price_series.index, price_series.values):\n",
    "    plt.text(\n",
    "        x, y+0.3, \n",
    "        f\"{y:.2f}\", \n",
    "        ha='center', \n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "for x, y in zip(forecast_mean.index, forecast_mean.values):\n",
    "    plt.text(\n",
    "        x, y+0.3, \n",
    "        f\"{y:.2f}\", \n",
    "        ha='center', \n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average price (USD)\")\n",
    "plt.title(\"Average game price over time using ARIMA forecast\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(list(price_series.index) + future_years, rotation=45)\n",
    "plt.ylim(0, max(price_series.values.max(), forecast_mean.values.max())+1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38f7741-776d-41f7-91bf-20a463cae2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring games by popularuty (estimated number of owners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ed8db-6e5e-4f0e-8146-2cde8d306f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_steam_fixed\n",
    "        .select(\"Estimated owners\")\n",
    "        .distinct()\n",
    "        .orderBy(\"Estimated owners\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f49feb0-1935-4f71-82db-b5ef0b26bc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "owner_bins = [\n",
    "    \"0 - 0\",\n",
    "    \"0 - 20000\",\n",
    "    \"20000 - 50000\",\n",
    "    \"50000 - 100000\",\n",
    "    \"100000 - 200000\",\n",
    "    \"200000 - 500000\",\n",
    "    \"500000 - 1000000\",\n",
    "    \"1000000 - 2000000\",\n",
    "    \"2000000 - 5000000\",\n",
    "    \"5000000 - 10000000\",\n",
    "    \"10000000 - 20000000\",\n",
    "    \"20000000 - 50000000\",\n",
    "    \"50000000 - 100000000\",\n",
    "    \"100000000 - 200000000\",\n",
    "]\n",
    "\n",
    "df_owners = (\n",
    "    df_steam_fixed\n",
    "        .groupBy(\"Estimated owners\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "owners_pd = df_owners.toPandas()\n",
    "cat_type = pd.CategoricalDtype(categories=owner_bins, ordered=True)\n",
    "owners_pd[\"Estimated owners\"] = owners_pd[\"Estimated owners\"].astype(cat_type)\n",
    "\n",
    "owners_pd = owners_pd.sort_values(\"Estimated owners\")\n",
    "owners_pd = owners_pd.dropna(subset=[\"Estimated owners\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(owners_pd[\"Estimated owners\"].astype(str), owners_pd[\"count\"])\n",
    "\n",
    "plt.xlabel(\"Estimated number of owners\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.title(\"Distribution of games by estimated number of owners\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e59d4f7-b56b-4c9f-b620-68c3bcc14ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_groups = (\n",
    "    df_steam_fixed\n",
    "        .withColumn(\n",
    "            \"Popularity group\",\n",
    "            F.when(F.col(\"Estimated owners\") == \"0 - 0\", \"Unplayed\")\n",
    "             .when(F.col(\"Estimated owners\").isin(\"0 - 20000\", \"20000 - 50000\"), \"Niche\")\n",
    "             .when(F.col(\"Estimated owners\").isin(\n",
    "                    \"50000 - 100000\",\n",
    "                    \"100000 - 200000\",\n",
    "                    \"200000 - 500000\"\n",
    "                  ), \"Mid-scale\")\n",
    "             .when(F.col(\"Estimated owners\").isin(\n",
    "                    \"500000 - 1000000\",\n",
    "                    \"1000000 - 2000000\",\n",
    "                    \"2000000 - 5000000\",\n",
    "                    \"5000000 - 10000000\",\n",
    "                    \"10000000 - 20000000\",\n",
    "                    \"20000000 - 50000000\",\n",
    "                    \"50000000 - 100000000\",\n",
    "                    \"100000000 - 200000000\"\n",
    "                  ), \"Hit\")\n",
    "             .otherwise(None)\n",
    "        )\n",
    "        .filter(F.col(\"Popularity group\").isNotNull())\n",
    ")\n",
    "\n",
    "df_group_counts = (\n",
    "    df_groups\n",
    "        .groupBy(\"Popularity group\")\n",
    "        .count()\n",
    "        .toPandas()\n",
    ")\n",
    "\n",
    "\n",
    "order = [\"Unplayed\", \"Niche\", \"Mid-scale\", \"Hit\"]\n",
    "df_group_counts = df_group_counts.set_index(\"Popularity group\").reindex(order).reset_index()\n",
    "\n",
    "groups = df_group_counts[\"Popularity group\"]\n",
    "counts = df_group_counts[\"count\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(groups, counts)\n",
    "\n",
    "# add labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        str(int(height)),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Popularity group (by estimated owners)\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.title(\"Distribution of games by popularity\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f832938a-ce17-4964-91d8-430e8c8a26c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's look into games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd1b4bc-51f6-4787-9f81-3a2ca85cfb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_top_owners = (\n",
    "    df_steam_fixed\n",
    "        .filter(F.col(\"Estimated owners\") == \"100000000 - 200000000\")\n",
    "        .select(\"Name\", \"Estimated owners\", \"Release date\")\n",
    "        .orderBy(F.col(\"Release date\").desc())\n",
    "        .limit(15)\n",
    ")\n",
    "\n",
    "display(df_top_owners)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2dc403-3c6e-470b-a0e8-c03702b7029c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see, there is only one game in Steam that crossed 100000000 users and it is Dota2\n",
    "\n",
    "Now, let's explore games with a bit smaller number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a62fa7-6075-4ad1-9950-78d298442387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_top_owners = (\n",
    "    df_steam_fixed\n",
    "        .filter(F.col(\"Estimated owners\") == \"50000000 - 100000000\")\n",
    "        .select(\"Name\", \"Estimated owners\", \"Release date\", \"Developers\")\n",
    "        .orderBy(F.col(\"Release date\").desc())\n",
    "        .limit(15)\n",
    ")\n",
    "\n",
    "display(df_top_owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a8c086-d729-48f9-8ee3-285edd7b9ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are not many games that crossed 50000000 users too. Among them we see popular shooter games like PUBG and CS-GO, but also relatively new games like Black Myth: Wukong and New World\n",
    "\n",
    "Black Myth: Wukong is an action game based on chinese mythology \n",
    "\n",
    "New World - supernatural RPG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26cb5c9-f893-4f36-ada0-3e92fa31349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_top_owners = (\n",
    "    df_steam_fixed\n",
    "        .filter(F.col(\"Estimated owners\") == \"20000000 - 50000000\")\n",
    "        .select(\"Name\", \"Estimated owners\", \"Genres\", \"Release date\", \"Developers\")\n",
    "        .orderBy(F.col(\"Release date\").desc())\n",
    "        .limit(30)\n",
    ")\n",
    "\n",
    "display(df_top_owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dccfc1da-1042-48e9-855f-663dde1e9372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploring games by reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322e4e54-7804-4b4c-88a6-7a975516d439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top5_positive = (\n",
    "    df_steam_fixed\n",
    "        .orderBy(F.col(\"Positive\").desc())\n",
    "        .select(\"Name\", \"Positive\", \"Negative\", \"Developers\", \"Release date\")\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "display(top5_positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210bceea-ec2f-4be7-bccd-29a3c23a3d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Predictably, one of the most popular games get the most positive reviews. Here we see mostly online shooter games (CS-GO, Dota, PUBG), but also open-world GTA 5 and Terraria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5960f6d6-b29a-48de-8308-19a66cbad900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top5_negative = (\n",
    "    df_steam_fixed\n",
    "        .orderBy(F.col(\"Negative\").desc())\n",
    "        .select(\"Name\", \"Positive\", \"Negative\", \"Developers\", \"Release date\")\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "display(top5_negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c37af8e-2cf3-4660-a113-cdad70af7eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Naturally, most popular games also get more negative reviews, so our top 5 is almost the same, except the Terraria, which was replaced by Tom Clancy's Rainbow Six Siege - another obline RPG game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49940635-b674-443a-9e5e-05e0a797d9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_devs = df_steam_fixed.withColumn(\n",
    "    \"Developers_array\",\n",
    "    F.split(F.regexp_replace(F.col(\"Developers\"), r\"\\s*,\\s*\", \",\"), \",\")\n",
    ")\n",
    "\n",
    "df_devs = df_devs.withColumn(\n",
    "    \"Developers_array\",\n",
    "    F.expr(\"transform(Developers_array, x -> trim(x))\")\n",
    ")\n",
    "\n",
    "df_dev_positive = (\n",
    "    df_devs\n",
    "        .withColumn(\"developer\", F.explode(\"Developers_array\"))\n",
    "        .filter(\n",
    "            (F.col(\"developer\").isNotNull()) &\n",
    "            (F.col(\"developer\") != \"\") & (F.length(\"developer\") > 4)\n",
    "        )\n",
    "        .groupBy(\"developer\")\n",
    "        .agg(F.sum(\"Positive\").alias(\"total_positive\"))\n",
    "        .orderBy(F.col(\"total_positive\").desc())\n",
    "        .limit(5)\n",
    ")\n",
    "df_dev_negative = (\n",
    "    df_devs\n",
    "        .withColumn(\"developer\", F.explode(\"Developers_array\"))\n",
    "        .filter(\n",
    "            (F.col(\"developer\").isNotNull()) &\n",
    "            (F.col(\"developer\") != \"\") & (F.length(\"developer\") > 4)\n",
    "        )\n",
    "        .groupBy(\"developer\")\n",
    "        .agg(F.sum(\"Negative\").alias(\"total_negative\"))\n",
    "        .orderBy(F.col(\"total_negative\").desc())\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "display(df_dev_positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a1187b-6c8f-4051-84d8-ff2d8fbb3bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When it comes to developers and positive reviews, Valve is the absolute leader. \n",
    "* Valve - CS-GO and Half life developer\n",
    "* Hidden Path Entertainment - CS-GO and Defence Grid (works with Valve)\n",
    "* Ubisoft Montreal - Assasin's Creed, Far Cry\n",
    "* Feral Interactive (Mac) - Hitman, Lara Croft games, Warhammer\n",
    "* Facepunch Studios - creators of Rust, Garry's mod (works with Valve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9991282d-b957-4354-b6f1-462dd71e7fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_dev_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a54a51-6462-44a1-b43e-ae45f5e65bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Similarly to the case with games, most popular studios also have most of the negative reviews. Still, we've got new leader in negative reviews KRAFTON - creators of PUBG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2aa8907-2019-4a06-a0b1-a451fc795df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Top games by other criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d4c510-8e05-4f28-a501-e1756c14dd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#by Metacritic score\n",
    "top5_metacritic = (\n",
    "    df_steam_fixed\n",
    "        .filter(F.col(\"Metacritic score\").isNotNull())\n",
    "        .orderBy(F.col(\"Metacritic score\").desc())\n",
    "        .select(\n",
    "            \"Name\",\n",
    "            \"Metacritic score\",\n",
    "            \"Estimated owners\",\n",
    "            \"Release date\",\n",
    "            \"Positive\",\n",
    "            \"Negative\"\n",
    "        )\n",
    "        .limit(10)\n",
    ")\n",
    "\n",
    "display(top5_metacritic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311a03c6-4cc5-4f6a-8981-c0fbffc49d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's look into this top\n",
    "* Disco Elysium - The Final Cut - detective role game \n",
    "* Persona 5 Royal - japanese role game\n",
    "* Half-Life - action shooter\n",
    "* Grand Theft Auto V - famous open-world game)\n",
    "* BioShock - well known horror action shooter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4058216c-0750-4fa9-8c79-902dfdf7cd5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# By number of achievements \n",
    "\n",
    "top5_achievements = (\n",
    "    df_steam_fixed\n",
    "        .orderBy(F.col(\"Achievements\").desc())\n",
    "        .select(\"Name\", \"Achievements\", \"Average playtime forever\", \"Release date\")\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "display(top5_achievements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285a57bb-52a2-47d2-b9d3-855e1cf98f27",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Name\":220},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765293188996}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# by Average playtime forever\n",
    "top5_playtime = (\n",
    "    df_steam_fixed\n",
    "        .orderBy(F.col(\"Average playtime forever\").desc())\n",
    "        .select(\"Name\", \"Average playtime forever\", \"Price\", \"Genres\", \"Release date\")\n",
    "        .limit(5)\n",
    ")\n",
    "\n",
    "display(top5_playtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fdaaa1-47e9-43a7-91d1-83c3fbd68697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We got quite interesting top of games by the average playtime, so let's look into them\n",
    "* Boom 3D - equalizer that makes game sounds 3D\n",
    "* Energy Engine PC Live Wallpaper - live walpapers\n",
    "* -Tlicolity Eyes- - Japanese visual novella, dating simulator\n",
    "* Defense Clicker - free strategic game\n",
    "* Relive - free game created to raise CPR averness and encourage users take CPR trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed2b534-3b2d-495b-82f0-9cee8d17237d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0cd365-1eb2-4740-8a1d-bb3973ee5325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS steam_project.bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS steam_project.silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS steam_project.gold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea97e03-4230-4c84-b834-f9f7ac715df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = (\n",
    "    df_steam_fixed\n",
    "    .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "    .withColumn(\"_source\", F.lit(\"steam_games_csv_v1\"))\n",
    ")\n",
    "\n",
    "(\n",
    "    df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.bronze.steam_games_raw\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5084932-6523-45c1-a647-965d55d1c534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_tbl = spark.table(\"steam_project.bronze.steam_games_raw\")\n",
    "\n",
    "print(\"Bronze sample\")\n",
    "df_bronze_tbl.show(5, truncate=False)\n",
    "print(\"\\nBronze schema\")\n",
    "df_bronze_tbl.printSchema()\n",
    "\n",
    "\n",
    "rows_source       = df_steam_fixed.count()\n",
    "rows_bronze_mem   = df_bronze.count()\n",
    "rows_bronze_table = df_bronze_tbl.count()\n",
    "\n",
    "print(\"\\nRow count check\")\n",
    "print(f\"df_steam_fixed   : {rows_source}\")\n",
    "print(f\"df_bronze (mem)  : {rows_bronze_mem}\")\n",
    "print(f\"bronze table     : {rows_bronze_table}\")\n",
    "\n",
    "assert rows_source == rows_bronze_mem,   \"Mismatch between df_steam_fixed and df_bronze\"\n",
    "assert rows_source == rows_bronze_table, \"Mismatch between df_steam_fixed and bronze table\"\n",
    "\n",
    "print(\"Row-count test passed: all row counts match.\")\n",
    "\n",
    "orig_cols   = set(df_steam_fixed.columns)\n",
    "bronze_cols = set(df_bronze_tbl.columns) - {\"_ingest_ts\", \"_source\"}  # ignore metadata\n",
    "\n",
    "missing_in_bronze = orig_cols - bronze_cols\n",
    "extra_in_bronze   = bronze_cols - orig_cols\n",
    "\n",
    "print(\"\\nColumn presence check\")\n",
    "print(\"Missing in Bronze:\", missing_in_bronze)\n",
    "print(\"Extra in Bronze  :\", extra_in_bronze)\n",
    "\n",
    "assert not missing_in_bronze, \"Some source columns are missing in Bronze!\"\n",
    "print(\"Column presence test passed: all source columns exist in Bronze.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bde0b9-c5ba-4589-95ed-558f24870669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097c811c-84a0-45b7-a2a6-a5914b7f2733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import or_\n",
    "\n",
    "\n",
    "df_bronze = spark.table(\"steam_project.bronze.steam_games_raw\")\n",
    "df_silver = df_bronze\n",
    "\n",
    "NULL_PATTERNS = [\n",
    "    r\"^$\",              # empty string\n",
    "    r\"^\\s+$\",           # whitespace\n",
    "    r\"(?i)^null$\",      # null / NULL\n",
    "    r\"(?i)^none$\",      # none / NONE\n",
    "    r\"(?i)^na$\",        # na / NA\n",
    "    r\"(?i)^n/a$\",       # n/a / N/A\n",
    "    r\"^\\?$\",            # ?\n",
    "]\n",
    "\n",
    "string_cols = [c for c, t in df_silver.dtypes if t == \"string\"]\n",
    "\n",
    "for c in string_cols:\n",
    "    cond = reduce(or_, [F.col(c).rlike(p) for p in NULL_PATTERNS])\n",
    "    df_silver = df_silver.withColumn(\n",
    "        c,\n",
    "        F.when(cond, None).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "\n",
    "array_cols_non_genres = [\n",
    "    \"Supported languages\",\n",
    "    \"Full audio languages\",\n",
    "    \"Categories\",\n",
    "    \"Tags\",\n",
    "]\n",
    "\n",
    "null_like_cond_expr = \" OR \".join([f\"x rlike '{p}'\" for p in NULL_PATTERNS])\n",
    "\n",
    "for c in array_cols_non_genres:\n",
    "    if c not in df_silver.columns:\n",
    "        continue\n",
    "\n",
    "    df_silver = df_silver.withColumn(\n",
    "        c,\n",
    "        F.when(\n",
    "            F.col(c).isNull(),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            F.expr(f\"filter(`{c}`, x -> NOT ({null_like_cond_expr}))\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_silver = df_silver.withColumn(\n",
    "        c,\n",
    "        F.when(F.size(F.col(c)) == 0, None).otherwise(F.col(c))\n",
    "    )\n",
    "\n",
    "\n",
    "df_silver_wo_genres = df_silver.drop(\"Genres\")\n",
    "\n",
    "null_like_cond_g = reduce(\n",
    "    or_,\n",
    "    [F.col(\"g\").rlike(p) for p in NULL_PATTERNS]\n",
    ")\n",
    "\n",
    "df_genres_clean = (\n",
    "    df_bronze\n",
    "    .select(\"AppID\", \"Genres\")\n",
    "    .withColumn(\"g\", F.explode_outer(\"Genres\"))\n",
    "    .withColumn(\"g\", F.regexp_replace(\"g\", r\"^\\s+|\\s+$\", \"\"))\n",
    "    .filter(~null_like_cond_g | F.col(\"g\").isNull())\n",
    "    .groupBy(\"AppID\")\n",
    "    .agg(\n",
    "        F.collect_list(\"g\").alias(\"Genres\")\n",
    "    )\n",
    "\n",
    "    .withColumn(\n",
    "        \"Genres\",\n",
    "        F.when(\n",
    "            (F.col(\"Genres\").isNull()) | (F.size(F.col(\"Genres\")) == 0),\n",
    "            None\n",
    "        ).otherwise(\n",
    "            F.expr(\"filter(Genres, x -> x is not null and trim(x) != '')\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "df_silver_final = (\n",
    "    df_silver_wo_genres\n",
    "    .join(df_genres_clean, on=\"AppID\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"Sample Silver with fixed Genres:\")\n",
    "df_silver_final.select(\"AppID\", \"Genres\").show(5, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "(\n",
    "    df_silver_final.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.silver.steam_games_clean\")\n",
    ")\n",
    "\n",
    "print(\"Silver overwritten with cleaned strings/arrays and fixed Genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4901305-cb19-4437-87f8-d09238854f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_silver = spark.table(\"steam_project.silver.steam_games_clean\")\n",
    "\n",
    "print(\"Silver non-null Genres:\", df_silver.filter(F.col(\"Genres\").isNotNull()).count())\n",
    "\n",
    "df_silver.select(\n",
    "    F.explode(\"Genres\").alias(\"genre_raw\")\n",
    ").withColumn(\n",
    "    \"genre_trimmed\", F.trim(\"genre_raw\")\n",
    ").groupBy(\"genre_trimmed\").count().orderBy(F.desc(\"count\")).show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8cc58c-4482-4b0e-bb57-2c07572cf282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import or_\n",
    "\n",
    "NULL_PATTERNS = [\n",
    "    r\"^$\",              \n",
    "    r\"^\\s+$\",           \n",
    "    r\"(?i)^null$\",      \n",
    "    r\"(?i)^none$\",      \n",
    "    r\"(?i)^na$\",        \n",
    "    r\"(?i)^n/a$\",       \n",
    "    r\"^\\?$\",            \n",
    "]\n",
    "\n",
    "array_cols = [\n",
    "    \"Supported languages\",\n",
    "    \"Full audio languages\",\n",
    "    \"Categories\",\n",
    "    \"Genres\",\n",
    "    \"Tags\",\n",
    "]\n",
    "\n",
    "df_bronze = spark.table(\"steam_project.bronze.steam_games_raw\")\n",
    "df_silver_tbl = spark.table(\"steam_project.silver.steam_games_clean\")\n",
    "\n",
    "rows_bronze = df_bronze.count()\n",
    "rows_silver = df_silver_tbl.count()\n",
    "print(f\"Rows Bronze: {rows_bronze}\")\n",
    "print(f\"Rows Silver: {rows_silver}\")\n",
    "assert rows_bronze == rows_silver, \"Row count mismatch between Bronze and Silver!\"\n",
    "print(\"Row-count test passed (Bronze == Silver)\")\n",
    "\n",
    "string_cols = [c for c, t in df_silver_tbl.dtypes if t == \"string\"]\n",
    "\n",
    "cond_all = None\n",
    "for c in string_cols:\n",
    "    col_cond = reduce(or_, [F.col(c).rlike(p) for p in NULL_PATTERNS])\n",
    "    cond_all = col_cond if cond_all is None else (cond_all | col_cond)\n",
    "\n",
    "remaining_str = df_silver_tbl.filter(cond_all).count() if cond_all is not None else 0\n",
    "print(\"Remaining null-like strings:\", remaining_str)\n",
    "assert remaining_str == 0, \"There are still null-like strings in Silver!\"\n",
    "print(\"All null-like string values removed in Silver\")\n",
    "\n",
    "array_cols_present = [c for c in array_cols if c in df_silver_tbl.columns]\n",
    "if array_cols_present:\n",
    "    empty_arrays = df_silver_tbl.filter(\n",
    "        reduce(or_, [F.size(F.col(c)) == 0 for c in array_cols_present])\n",
    "    ).count()\n",
    "else:\n",
    "    empty_arrays = 0\n",
    "\n",
    "print(\"Empty arrays remaining:\", empty_arrays)\n",
    "assert empty_arrays == 0, \"Some empty arrays remain in Silver!\"\n",
    "print(\"No empty arrays left in Silver\")\n",
    "\n",
    "print(\"Silver cleaning complete and tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2204790-b3fb-4a50-b91c-dec2bb0ab500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf73de8-0ed9-439f-bbb6-f03437c9cf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_silver = spark.table(\"steam_project.silver.steam_games_clean\")\n",
    "\n",
    "print(\"Rows in Silver:\", df_silver.count())\n",
    "\n",
    "df_price_stats_gold = (\n",
    "    df_silver\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .filter(\n",
    "            (F.col(\"Release date\").isNotNull()) &\n",
    "            (F.col(\"year\").between(2010, 2024)) &\n",
    "            (F.col(\"Price\").isNotNull()) &\n",
    "            (F.col(\"Price\") > 0)\n",
    "        )\n",
    "        .groupBy(\"year\")\n",
    "        .agg(\n",
    "            F.mean(\"Price\").alias(\"avg_price\"),\n",
    "            F.expr(\"percentile_approx(Price, 0.5)\").alias(\"median_price\"),\n",
    "            F.min(\"Price\").alias(\"min_price\"),\n",
    "            F.max(\"Price\").alias(\"max_price\"),\n",
    "            F.count(\"*\").alias(\"num_games\")\n",
    "        )\n",
    "        .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_price_stats_gold.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.gold.games_price_stats_by_year\")\n",
    ")\n",
    "\n",
    "print(\"GOLD 1 written: steam_project.gold.games_price_stats_by_year\")\n",
    "\n",
    "g1 = spark.table(\"steam_project.gold.games_price_stats_by_year\")\n",
    "print(\"GOLD 1 rows:\", g1.count())\n",
    "g1.show(5)\n",
    "\n",
    "min_year = g1.select(F.min(\"year\")).first()[0]\n",
    "max_year = g1.select(F.max(\"year\")).first()[0]\n",
    "assert 2010 <= min_year <= 2024, \"Min year in GOLD 1 out of expected range\"\n",
    "assert 2010 <= max_year <= 2024, \"Max year in GOLD 1 out of expected range\"\n",
    "print(\"GOLD 1 tests passed\")\n",
    "\n",
    "target_genres = [\"Action\", \"Indie\", \"Casual\"]\n",
    "\n",
    "df_genre_trends_gold = (\n",
    "    df_silver\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .filter(F.col(\"year\").isNotNull())\n",
    "        .withColumn(\"genre_raw\", F.explode(\"Genres\"))      # explode array\n",
    "        .withColumn(\"genre\", F.trim(F.col(\"genre_raw\")))   # trim spaces\n",
    "        .filter(F.col(\"genre\").isin(target_genres))\n",
    "        .groupBy(\"year\", \"genre\")\n",
    "        .count()\n",
    "        .orderBy(\"year\", \"genre\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_genre_trends_gold.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.gold.games_genre_trends_key_genres\")\n",
    ")\n",
    "\n",
    "print(\"GOLD 2 written: steam_project.gold.games_genre_trends_key_genres\")\n",
    "\n",
    "g2 = spark.table(\"steam_project.gold.games_genre_trends_key_genres\")\n",
    "print(\"GOLD 2 rows:\", g2.count())\n",
    "g2.show(20)\n",
    "\n",
    "genres_in_table = [r[0] for r in g2.select(\"genre\").distinct().collect()]\n",
    "unexpected_genres = set(genres_in_table) - set(target_genres)\n",
    "assert not unexpected_genres, f\"GOLD 2 contains unexpected genres: {unexpected_genres}\"\n",
    "print(\"GOLD 2 tests passed\")\n",
    "\n",
    "df_fp_year = (\n",
    "    df_silver\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .filter(F.col(\"year\").isNotNull())\n",
    "        .withColumn(\"type\", F.when(F.col(\"Price\") == 0, \"Free\").otherwise(\"Paid\"))\n",
    "        .groupBy(\"year\", \"type\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_fp_share_gold = (\n",
    "    df_fp_year\n",
    "        .groupBy(\"year\")\n",
    "        .pivot(\"type\", [\"Free\", \"Paid\"])\n",
    "        .sum(\"count\")\n",
    "        .fillna(0)\n",
    "        .withColumn(\"total\", F.col(\"Free\") + F.col(\"Paid\"))\n",
    "        .withColumn(\"pct_free\", F.round(F.col(\"Free\") / F.col(\"total\"), 3))\n",
    "        .withColumn(\"pct_paid\", F.round(F.col(\"Paid\") / F.col(\"total\"), 3))\n",
    "        .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_fp_share_gold.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.gold.games_free_vs_paid_share\")\n",
    ")\n",
    "\n",
    "print(\"GOLD 3 written: steam_project.gold.games_free_vs_paid_share\")\n",
    "\n",
    "g3 = spark.table(\"steam_project.gold.games_free_vs_paid_share\")\n",
    "print(\"GOLD 3 rows:\", g3.count())\n",
    "g3.show(5)\n",
    "\n",
    "max_diff = g3.select(F.max(F.abs(F.col(\"pct_free\") + F.col(\"pct_paid\") - 1))).first()[0]\n",
    "assert max_diff < 1e-6, \"pct_free + pct_paid != 1 for some years in GOLD 3\"\n",
    "print(\"GOLD 3 tests passed\")\n",
    "\n",
    "df_quarter_gold = (\n",
    "    df_silver\n",
    "        .withColumn(\"year\", F.year(\"Release date\"))\n",
    "        .withColumn(\"quarter\", F.quarter(\"Release date\"))\n",
    "        .filter(F.col(\"year\").isNotNull() & F.col(\"quarter\").isNotNull())\n",
    "        .groupBy(\"year\", \"quarter\")\n",
    "        .count()\n",
    "        .orderBy(\"year\", \"quarter\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_quarter_gold.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.columnMapping.mode\", \"name\")\n",
    "    .option(\"delta.minReaderVersion\", \"2\")\n",
    "    .option(\"delta.minWriterVersion\", \"5\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"steam_project.gold.games_releases_by_quarter\")\n",
    ")\n",
    "\n",
    "print(\"GOLD 4 written: steam_project.gold.games_releases_by_quarter\")\n",
    "\n",
    "g4 = spark.table(\"steam_project.gold.games_releases_by_quarter\")\n",
    "print(\"GOLD 4 rows:\", g4.count())\n",
    "g4.show(5)\n",
    "\n",
    "quarters = [r[0] for r in g4.select(\"quarter\").distinct().collect()]\n",
    "assert set(quarters).issubset({1, 2, 3, 4}), f\"Unexpected quarter values in GOLD 4: {quarters}\"\n",
    "print(\"GOLD 4 tests passed\")\n",
    "\n",
    "print(\"All GOLD tables created and validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495a2707-f711-4525-8590-43ceef232827",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Genres\":309},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Header image\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Website\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Metacritic url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"Screenshots\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1765221630559}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_bronze = spark.table(\"steam_project.bronze.steam_games_raw\")\n",
    "\n",
    "df_bronze_1997 = (\n",
    "    df_bronze\n",
    "        .filter(F.year(F.col(\"Release date\")) == 1997)\n",
    ")\n",
    "\n",
    "display(df_bronze_1997)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
